{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:30:04.273098Z","iopub.status.busy":"2024-07-23T17:30:04.272745Z","iopub.status.idle":"2024-07-23T17:30:05.845537Z","shell.execute_reply":"2024-07-23T17:30:05.844532Z","shell.execute_reply.started":"2024-07-23T17:30:04.273070Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'NLP-Corpus'...\n","remote: Enumerating objects: 1223, done.\u001b[K\n","remote: Counting objects: 100% (322/322), done.\u001b[K\n","remote: Compressing objects: 100% (220/220), done.\u001b[K\n","remote: Total 1223 (delta 290), reused 102 (delta 102), pack-reused 901\u001b[K\n","Receiving objects: 100% (1223/1223), 2.55 MiB | 19.95 MiB/s, done.\n","Resolving deltas: 100% (566/566), done.\n","/kaggle/working/NLP-Corpus/Pipeline\n"]}],"source":["!git clone https://github.com/Falgun1/NLP-Corpus\n","%cd NLP-Corpus/Pipeline\n","# !ls -R\n","# !ls -R NLP-Corpus"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:28:59.475672Z","iopub.status.busy":"2024-07-23T17:28:59.475012Z","iopub.status.idle":"2024-07-23T17:29:40.919117Z","shell.execute_reply":"2024-07-23T17:29:40.917848Z","shell.execute_reply.started":"2024-07-23T17:28:59.475644Z"},"trusted":true},"outputs":[],"source":["%%capture\n","import torch\n","!pip install bitsandbytes\n","!pip install datasets\n","major_version, minor_version = torch.cuda.get_device_capability()\n","if major_version >= 8:\n","    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n","else:\n","    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n","pass"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:32:02.670611Z","iopub.status.busy":"2024-07-23T17:32:02.670195Z","iopub.status.idle":"2024-07-23T17:32:18.020812Z","shell.execute_reply":"2024-07-23T17:32:18.019824Z","shell.execute_reply.started":"2024-07-23T17:32:02.670581Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-23 17:32:07.222861: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-23 17:32:07.222961: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-23 17:32:07.356580: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import torch, os, json, random, bitsandbytes as bnb, torch.nn as nn, psutil\n","from datasets import Dataset, DatasetDict, load_dataset\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, TrainingArguments, BitsAndBytesConfig\n","from trl import SFTTrainer\n","import re\n","from pprint import pprint\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\n","import pandas as pd "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:33:00.503282Z","iopub.status.busy":"2024-07-23T17:33:00.502079Z","iopub.status.idle":"2024-07-23T17:35:36.326984Z","shell.execute_reply":"2024-07-23T17:35:36.325958Z","shell.execute_reply.started":"2024-07-23T17:33:00.503236Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading tokenizer...\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:778: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be8ea874439942d8875800e8803ee5ef","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48515ff2effd4fb9987278dc4222d819","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"41205ffcd13f48a0bba68b4243035752","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["EOS Token: <|end_of_text|>\n","EOS Token ID: 128001\n","Loading model...\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4592666b73eb4653b6f658dce35edad5","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc44db5101ba46c88475c24c7f574cc3","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f6e094a5ef74d3f9bf863b74f428c06","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a79e37ca58e14fa69312dc070ab86d1e","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57959972f48f4f138f36c0f8c7eea37a","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e725c3c50354c27b75320d91aef2edd","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8841b497b6c040ae9edee6f604570ae5","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf7fdb986d0d4f90874f20de048a1e5e","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db763cf30a0f4174aeedc9e7d7d490ac","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Applying gradient checkpointing and preparing for k-bit training...\n","Model and tokenizer loaded and configured successfully.\n","Applying LoRA configuration...\n","LoRA configuration applied successfully.\n"]}],"source":["HF_TOKEN = \"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\"\n","model_name = \"meta-llama/Meta-Llama-3-8B\"\n","max_seq_length = 2048\n","\n","def load_model_and_tokenizer():\n","    try:\n","        print(\"Loading tokenizer...\")\n","        tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=HF_TOKEN)\n","        tokenizer.pad_token = tokenizer.eos_token\n","        tokenizer.padding_side = \"right\"\n","\n","        special_tokens = tokenizer.special_tokens_map_extended\n","        eos_token = tokenizer.eos_token\n","        eos_token_id = tokenizer.eos_token_id\n","\n","        print(\"EOS Token:\", eos_token)\n","        print(\"EOS Token ID:\", eos_token_id)\n","\n","        # Configure Quantization\n","        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n","\n","        # Load Pretrained Model with Quantization\n","        print(\"Loading model...\")\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_name,\n","            quantization_config=quantization_config,\n","            device_map='auto',  # Automatically distribute to CPU and GPU\n","            low_cpu_mem_usage=True,\n","            use_auth_token=HF_TOKEN\n","        )\n","\n","        # Enable Gradient Checkpointing and Prepare for k-bit Training\n","        print(\"Applying gradient checkpointing and preparing for k-bit training...\")\n","        model.gradient_checkpointing_enable()\n","        model = prepare_model_for_kbit_training(model)\n","\n","        print(\"Model and tokenizer loaded and configured successfully.\")\n","        return model, tokenizer\n","\n","    except Exception as e:\n","        print(\"An error occurred:\", e)\n","\n","# Load the model and tokenizer\n","model, tokenizer = load_model_and_tokenizer()\n","\n","def apply_lora_config(model):\n","    try:\n","        print(\"Applying LoRA configuration...\")\n","\n","        # Define LoRA configuration\n","        lora_config = LoraConfig(\n","            r=16,\n","            lora_alpha=16,\n","            target_modules=[\"q_proj\", \"v_proj\"],\n","            lora_dropout=0.05,\n","            bias=\"none\",\n","            task_type=\"CAUSAL_LM\"\n","        )\n","\n","        # Apply LoRA configuration to the model\n","        model = get_peft_model(model, lora_config)\n","\n","        print(\"LoRA configuration applied successfully.\")\n","        return model\n","\n","    except Exception as e:\n","        print(\"An error occurred while applying LoRA configuration:\", e)\n","        return model\n","\n","# Apply LoRA configuration\n","model = apply_lora_config(model)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:43:12.921800Z","iopub.status.busy":"2024-07-23T17:43:12.920923Z","iopub.status.idle":"2024-07-23T17:43:12.936634Z","shell.execute_reply":"2024-07-23T17:43:12.935474Z","shell.execute_reply.started":"2024-07-23T17:43:12.921754Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["JSON files have been formatted and saved successfully.\n"]}],"source":["with open(\"qa_pairs.json\") as json_file:\n","    qa_pairs = json.load(json_file)   \n","with open(\"test.json\") as json_file:\n","    first_test_dataset = json.load(json_file)\n","    \n","def add_questions_key(input_file, output_file):\n","    # Read the JSON file\n","    with open(input_file, 'r') as infile:\n","        data = json.load(infile)        \n","    # Format the data by adding the 'questions' key\n","    formatted_data = {\n","        \"questions\": data\n","    }\n","\n","    # Write the formatted data to a new file\n","    with open(output_file, 'w') as outfile:\n","        json.dump(formatted_data, outfile, indent=4)\n","# File paths for JSON datasets\n","train_input_file = 'qa_pairs.json'\n","train_output_file = 'train_dataset.json'\n","\n","test_input_file = 'test.json'\n","test_output_file = 'test_dataset.json'\n","\n","# Convert training and test datasets to the appropriate format\n","add_questions_key(train_input_file, train_output_file)\n","add_questions_key(test_input_file, test_output_file)\n","\n","print(\"JSON files have been formatted and saved successfully.\")\n","\n","with open(\"test_dataset.json\") as json_file:\n","    test = json.load(json_file)    \n","with open(\"train_dataset.json\") as json_file:\n","    train = json.load(json_file)\n","# pd.DataFrame(train[\"questions\"]).head()\n","# pd.DataFrame(test[\"questions\"]).head()\n","# pprint(train[\"questions\"][0], sort_dicts=False)\n","# pprint(test[\"questions\"][0], sort_dicts=False)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-07-23T17:58:59.758739Z","iopub.status.busy":"2024-07-23T17:58:59.758368Z","iopub.status.idle":"2024-07-23T17:58:59.825976Z","shell.execute_reply":"2024-07-23T17:58:59.825076Z","shell.execute_reply.started":"2024-07-23T17:58:59.758709Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6541ac9034c348b598e4102736d16ae5","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/237 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f035d66c93b4d0eb025fdeb99238f84","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/25 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['question', 'answer'],\n","        num_rows: 237\n","    })\n","    test: Dataset({\n","        features: ['question', 'answer'],\n","        num_rows: 25\n","    })\n","})\n"]}],"source":["# Check if 'questions' key exists and if it has the required structure\n","def check_data_format(data):\n","    if \"questions\" not in data or not isinstance(data[\"questions\"], list):\n","        raise ValueError(\"The data does not contain the 'questions' key or it is not a list.\")\n","special_tokens = tokenizer.special_tokens_map_extended\n","eos_token = tokenizer.eos_token\n","eos_token_id = tokenizer.eos_token_id\n","        \n","check_data_format(train)\n","check_data_format(test)\n","\n","# Define the prompt format\n","prompt = \"\"\"Below is a question paired with an answer. Write a response that appropriately completes the request.\n","\n","### Question:\n","{}\n","\n","### Answer:\n","{}\"\"\"\n","\n","# Function to format the prompts\n","def formatting_prompts_func(examples):\n","    questions = examples[\"question\"]\n","    answers = examples[\"answer\"]\n","    texts = []\n","    for question, answer in zip(questions, answers):\n","        # Format the text as per the prompt\n","        text = prompt.format(question, answer) + eos_token\n","        texts.append(text)\n","    return {\"text\": texts}\n","\n","# Convert your data into a dataset\n","def create_and_format_dataset(data):\n","    dataset_dict = {\n","        \"question\": [item[\"question\"] for item in data[\"questions\"]],\n","        \"answer\": [item[\"answer\"] for item in data[\"questions\"]],\n","    }\n","    dataset = Dataset.from_dict(dataset_dict)\n","    # Apply the formatting prompts function to add 'text'\n","    dataset = dataset.map(formatting_prompts_func, batched=True)\n","    # Remove the 'text' column if it was added\n","    dataset = dataset.remove_columns([\"text\"])\n","    return dataset\n","\n","# Create and format datasets\n","train_dataset = create_and_format_dataset(train)\n","test_dataset = create_and_format_dataset(test)\n","\n","# Create a DatasetDict\n","dataset_dict = DatasetDict({\n","    'train': train_dataset,\n","    'test': test_dataset\n","})\n","\n","# Check the formatted dataset\n","print(dataset_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}

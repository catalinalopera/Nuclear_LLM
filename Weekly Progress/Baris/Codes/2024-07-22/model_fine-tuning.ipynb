{"cells":[{"cell_type":"markdown","metadata":{},"source":["# LLAMA3\n","https://ai.meta.com/blog/meta-llama-3/\n","\n","https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md"]},{"cell_type":"markdown","metadata":{},"source":["# 1. GPU detection to prevent version conflicts"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-21T00:05:26.795023Z","iopub.status.busy":"2024-07-21T00:05:26.794775Z","iopub.status.idle":"2024-07-21T00:06:08.650294Z","shell.execute_reply":"2024-07-21T00:06:08.648974Z","shell.execute_reply.started":"2024-07-21T00:05:26.794999Z"},"trusted":true},"outputs":[],"source":["%%capture\n","import torch\n","!pip install bitsandbytes\n","!pip install datasets\n","major_version, minor_version = torch.cuda.get_device_capability()\n","if major_version >= 8:\n","    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n","else:\n","    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n","pass"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Import Python Packages"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T00:08:24.215713Z","iopub.status.busy":"2024-07-21T00:08:24.215001Z","iopub.status.idle":"2024-07-21T00:08:40.051059Z","shell.execute_reply":"2024-07-21T00:08:40.049922Z","shell.execute_reply.started":"2024-07-21T00:08:24.215676Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-21 00:08:28.860705: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-21 00:08:28.860836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-21 00:08:29.004429: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import torch, os, json, random, bitsandbytes as bnb, torch.nn as nn, psutil\n","from datasets import Dataset, DatasetDict, load_dataset\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, TrainingArguments, BitsAndBytesConfig\n","from trl import SFTTrainer\n","import re\n","from pprint import pprint\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig"]},{"cell_type":"markdown","metadata":{},"source":["# 3. LLAMA 3 8B 8bit quantized"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T00:08:45.704612Z","iopub.status.busy":"2024-07-21T00:08:45.703433Z","iopub.status.idle":"2024-07-21T00:10:27.476132Z","shell.execute_reply":"2024-07-21T00:10:27.475179Z","shell.execute_reply.started":"2024-07-21T00:08:45.704577Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading tokenizer...\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:778: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40ef6407f7684cd3ba62d86cd629de58","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c727310aebc43bca20f4a2cae3506d3","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4839814061554b6cb841a3bdda8637be","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["EOS Token: <|end_of_text|>\n","EOS Token ID: 128001\n","Loading model...\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8f4a4d33c5b499f9143f5d8d6c6287f","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c10287e67e0745e1bbb2e2e9c34c2ab2","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9457e75b03d748819224d67c6ccdac73","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e35babdc1ac649f0bbb8a7f031bf0784","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82bf2e82588e47a6b44844c4e47464f1","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8783a9aefb17488d94b000841c39bbfd","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ffc33298f734e91aa58bf31bed99fb4","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ee6629ce6b447a8a07f1f9a84a502eb","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a19114ab79c94636b7cb37c0c519c952","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Applying gradient checkpointing and preparing for k-bit training...\n","Model and tokenizer loaded and configured successfully.\n"]}],"source":["HF_TOKEN = \"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\"\n","model_name = \"meta-llama/Meta-Llama-3-8B\"\n","max_seq_length = 2048\n","\n","def load_model_and_tokenizer():\n","    try:\n","        print(\"Loading tokenizer...\")\n","        tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=HF_TOKEN)\n","        tokenizer.pad_token = tokenizer.eos_token\n","        tokenizer.padding_side = \"right\"\n","\n","        special_tokens = tokenizer.special_tokens_map_extended\n","        eos_token = tokenizer.eos_token\n","        eos_token_id = tokenizer.eos_token_id\n","\n","        print(\"EOS Token:\", eos_token)\n","        print(\"EOS Token ID:\", eos_token_id)\n","\n","        # Configure Quantization\n","        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n","\n","        # Load Pretrained Model with Quantization\n","        print(\"Loading model...\")\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_name,\n","            quantization_config=quantization_config,\n","            device_map='auto',  # Automatically distribute to CPU and GPU\n","            low_cpu_mem_usage=True,\n","            use_auth_token=HF_TOKEN\n","        )\n","\n","        # Enable Gradient Checkpointing and Prepare for k-bit Training\n","        print(\"Applying gradient checkpointing and preparing for k-bit training...\")\n","        model.gradient_checkpointing_enable()\n","        model = prepare_model_for_kbit_training(model)\n","\n","        print(\"Model and tokenizer loaded and configured successfully.\")\n","        return model, tokenizer\n","\n","    except Exception as e:\n","        print(\"An error occurred:\", e)\n","\n","# Load the model and tokenizer\n","model, tokenizer = load_model_and_tokenizer()\n"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Lora Config and test"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T00:12:13.691876Z","iopub.status.busy":"2024-07-21T00:12:13.690938Z","iopub.status.idle":"2024-07-21T00:12:13.890782Z","shell.execute_reply":"2024-07-21T00:12:13.889643Z","shell.execute_reply.started":"2024-07-21T00:12:13.691841Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Applying LoRA configuration...\n","LoRA configuration applied successfully.\n"]}],"source":["from peft import LoraConfig, get_peft_model\n","\n","def apply_lora_config(model):\n","    try:\n","        print(\"Applying LoRA configuration...\")\n","\n","        # Define LoRA configuration\n","        lora_config = LoraConfig(\n","            r=16,\n","            lora_alpha=16,\n","            target_modules=[\"q_proj\", \"v_proj\"],\n","            lora_dropout=0.05,\n","            bias=\"none\",\n","            task_type=\"CAUSAL_LM\"\n","        )\n","\n","        # Apply LoRA configuration to the model\n","        model = get_peft_model(model, lora_config)\n","\n","        print(\"LoRA configuration applied successfully.\")\n","        return model\n","\n","    except Exception as e:\n","        print(\"An error occurred while applying LoRA configuration:\", e)\n","        return model\n","\n","# Apply LoRA configuration\n","model = apply_lora_config(model)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Data Preparation"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T00:12:17.951300Z","iopub.status.busy":"2024-07-21T00:12:17.950481Z","iopub.status.idle":"2024-07-21T00:12:34.585082Z","shell.execute_reply":"2024-07-21T00:12:34.584049Z","shell.execute_reply.started":"2024-07-21T00:12:17.951268Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n","Downloading...\n","From: https://drive.google.com/uc?id=1yl9K1M_Ey86DCU26jHX69v2GQhoURIcf\n","To: /kaggle/working/qa_pairs.json\n","100%|██████████| 43.0k/43.0k [00:00<00:00, 42.1MB/s]\n"]},{"data":{"text/plain":["'qa_pairs.json'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["!pip install -q gdown\n","import gdown\n","# Google Drive file ID\n","file_id = '1yl9K1M_Ey86DCU26jHX69v2GQhoURIcf'\n","# Local file path where the downloaded file will be saved\n","output_path = 'qa_pairs.json'\n","# Download the file from Google Drive\n","gdown.download(f'https://drive.google.com/uc?id={file_id}', output_path, quiet=False)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T00:12:41.898008Z","iopub.status.busy":"2024-07-21T00:12:41.896778Z","iopub.status.idle":"2024-07-21T00:12:41.974876Z","shell.execute_reply":"2024-07-21T00:12:41.974002Z","shell.execute_reply.started":"2024-07-21T00:12:41.897970Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aee16765820142469767ed535bfe4633","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/147 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'question': 'What is the name of the CNSC?', 'answer': 'The Canadian Nuclear Safety Commission (CNSC; French: Commission Canadienne de sûreté nucléaire) is the federal regulator of nuclear power and materials in Canada.', 'text': 'Below is a question paired with an answer. Write a response that appropriately completes the request.\\n\\n### Question:\\nWhat is the name of the CNSC?\\n\\n### Answer:\\nThe Canadian Nuclear Safety Commission (CNSC; French: Commission Canadienne de sûreté nucléaire) is the federal regulator of nuclear power and materials in Canada.<|end_of_text|>'}\n"]}],"source":["from datasets import Dataset\n","import json\n","from transformers import AutoTokenizer\n","special_tokens = tokenizer.special_tokens_map_extended\n","eos_token = tokenizer.eos_token\n","eos_token_id = tokenizer.eos_token_id\n","\n","# Load your data\n","with open(\"qa_pairs.json\") as json_file:\n","    data = json.load(json_file)\n","\n","# Check if 'questions' key exists and if it has the required structure\n","if \"questions\" not in data or not isinstance(data[\"questions\"], list):\n","    raise ValueError(\"The data does not contain the 'questions' key or it is not a list.\")\n","\n","# Define the prompt format\n","ecommerce_prompt = \"\"\"Below is a question paired with an answer. Write a response that appropriately completes the request.\n","\n","### Question:\n","{}\n","\n","### Answer:\n","{}\"\"\"\n","\n","# Function to format the prompts\n","def formatting_prompts_func(examples):\n","    questions = examples[\"question\"]\n","    answers = examples[\"answer\"]\n","    texts = []\n","    for question, answer in zip(questions, answers):\n","        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n","        text = ecommerce_prompt.format(question, answer) + eos_token\n","        texts.append(text)\n","    return {\"text\": texts}\n","\n","# Convert your data into a dataset and format it\n","dataset_dict = {\n","    \"question\": [item[\"question\"] for item in data[\"questions\"]],\n","    \"answer\": [item[\"answer\"] for item in data[\"questions\"]]\n","}\n","dataset = Dataset.from_dict(dataset_dict)\n","dataset = dataset.map(formatting_prompts_func, batched=True)\n","\n","# Check the formatted dataset\n","print(dataset[3])\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T23:01:25.817532Z","iopub.status.busy":"2024-07-20T23:01:25.816643Z","iopub.status.idle":"2024-07-20T23:01:25.824565Z","shell.execute_reply":"2024-07-20T23:01:25.823467Z","shell.execute_reply.started":"2024-07-20T23:01:25.817490Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Example 1:\n","Text: Below is a question paired with an answer. Write a response that appropriately completes the request.\n","\n","### Question:\n","What is the name of the CNSC?\n","\n","### Answer:\n","The CNSC is an agency of the Government of Canada which reports to the Parliament of Canada through the Minister of Natural Resources.<|end_of_text|>\n","\n","Example 2:\n","Text: Below is a question paired with an answer. Write a response that appropriately completes the request.\n","\n","### Question:\n","What is the role of the CNSC?\n","\n","### Answer:\n","The Participant Funding Program allows the public, Indigenous groups, and other stakeholders to request funding from the CNSC to participate in its regulatory processes.<|end_of_text|>\n","\n","Example 3:\n","Text: Below is a question paired with an answer. Write a response that appropriately completes the request.\n","\n","### Question:\n","When did she become the President and CEO?\n","\n","### Answer:\n","Rumina Velshi joined the organisation in 2011 and in 2018 she became the President and CEO.<|end_of_text|>\n","\n","Example 4:\n","Text: Below is a question paired with an answer. Write a response that appropriately completes the request.\n","\n","### Question:\n","What is the name of the CNSC?\n","\n","### Answer:\n","The Canadian Nuclear Safety Commission (CNSC; French: Commission Canadienne de sûreté nucléaire) is the federal regulator of nuclear power and materials in Canada.<|end_of_text|>\n","\n","Example 5:\n","Text: Below is a question paired with an answer. Write a response that appropriately completes the request.\n","\n","### Question:\n","What is the role of the CNSC?\n","\n","### Answer:\n","Canadian Nuclear Safety Commission was established under the 1997 Nuclear Safety and Control Act with a mandate to regulate nuclear energy, nuclear substances, and relevant equipment in order to reduce and manage the safety, environmental, and national security risks, and to keep Canada in compliance with international legal obligations, such as the Treaty on the Non-Proliferation of Nuclear Weapons.<|end_of_text|>\n","\n"]}],"source":["# # Check the formatted dataset\n","# for i in range(5):  # Show the first 5 examples\n","#     print(f\"Example {i + 1}:\")\n","#     print(f\"Text: {dataset[i]['text']}\")\n","#     print()"]},{"cell_type":"markdown","metadata":{},"source":["# 6. Training"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T01:37:36.250973Z","iopub.status.busy":"2024-07-21T01:37:36.250584Z","iopub.status.idle":"2024-07-21T01:57:06.536182Z","shell.execute_reply":"2024-07-21T01:57:06.535157Z","shell.execute_reply.started":"2024-07-21T01:37:36.250944Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["max_steps is given, it will override any value given in num_train_epochs\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [80/80 19:15, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.863800</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.829300</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.128400</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.111600</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.829300</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>1.023800</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>1.016700</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.070200</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.939100</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.175200</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>1.400500</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>1.053000</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.952400</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>1.047300</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>1.195300</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.608100</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.911800</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>1.013100</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>1.340700</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.484000</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>1.341300</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>1.347900</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.700500</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.932800</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.897300</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>1.164700</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.948500</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>1.163700</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.889600</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.145500</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.942700</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.964000</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>1.196200</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.864000</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.915100</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.819000</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>1.160800</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>1.171000</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.833000</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.850200</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.942200</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.788500</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.941800</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.959000</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>1.307500</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.477100</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>1.105500</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.952500</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>1.007400</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.768100</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.754700</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.747600</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>1.003500</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.871000</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>1.049600</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.604300</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.743000</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.785400</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.801100</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.751800</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>1.028100</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.949400</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.812300</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.773200</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.728600</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.826700</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.825400</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.728900</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.821800</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.586800</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.666400</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.792000</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.696800</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.748000</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.725500</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.854500</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.731400</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.738600</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.781300</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.759000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-669c6af2-36c2992a75d684825f9753fc;5bdaa6d7-c55b-4f28-aade-c416f3d15cda)\n","\n","Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n","Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n","  warnings.warn(\n"]},{"data":{"text/plain":["TrainOutput(global_step=80, training_loss=0.927222053706646, metrics={'train_runtime': 1169.7346, 'train_samples_per_second': 0.274, 'train_steps_per_second': 0.068, 'total_flos': 7384341298544640.0, 'train_loss': 0.927222053706646, 'epoch': 2.4242424242424243})"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n","\n","# Eğitim konfigürasyonu\n","OUTPUT_DIR = \"experiments\"\n","\n","training_args = TrainingArguments(\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=4,\n","    num_train_epochs=1,\n","    learning_rate=2e-4,\n","    fp16=True,\n","    save_total_limit=3,\n","    logging_steps=1,\n","    output_dir=OUTPUT_DIR,\n","    max_steps=80,\n","    optim=\"paged_adamw_8bit\",\n","    lr_scheduler_type=\"cosine\",\n","    warmup_ratio=0.05,\n","    report_to=\"tensorboard\",\n",")  \n","\n","# Trainer'ı oluşturma\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=tokenizer,\n","    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",")\n","\n","model.config.use_cache = False\n","# Modeli eğitme\n","trainer.train()\n"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T02:03:34.735804Z","iopub.status.busy":"2024-07-21T02:03:34.735379Z","iopub.status.idle":"2024-07-21T02:03:34.744339Z","shell.execute_reply":"2024-07-21T02:03:34.743152Z","shell.execute_reply.started":"2024-07-21T02:03:34.735773Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Log directory '/kaggle/working/experiments' exists.\n","Files in log directory:\n","['checkpoint-80', 'runs']\n"]}],"source":["import os\n","# check Log \n","log_dir = \"/kaggle/working/experiments\"  \n","if not os.path.exists(log_dir):\n","    print(f\"Log directory '{log_dir}' does not exist.\")\n","else:\n","    print(f\"Log directory '{log_dir}' exists.\")\n","    print(\"Files in log directory:\")\n","    print(os.listdir(log_dir))\n","# download logs, enter below codes to cmd to show details\n","# tensorboard --logdir=C:\\Users\\engba\\Desktop\\runs\\Jul21_01-35-02_ed8e81291c4a\n","# site : http://localhost:6007/"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T03:34:03.780454Z","iopub.status.busy":"2024-07-21T03:34:03.779854Z","iopub.status.idle":"2024-07-21T03:34:03.864658Z","shell.execute_reply":"2024-07-21T03:34:03.863486Z","shell.execute_reply.started":"2024-07-21T03:34:03.780422Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import login, HfApi\n","login(token=\"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\")"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T03:34:55.733124Z","iopub.status.busy":"2024-07-21T03:34:55.732161Z","iopub.status.idle":"2024-07-21T03:34:55.741709Z","shell.execute_reply":"2024-07-21T03:34:55.740719Z","shell.execute_reply.started":"2024-07-21T03:34:55.733080Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"quantization_config\": {\n","    \"_load_in_4bit\": false,\n","    \"_load_in_8bit\": true,\n","    \"bnb_4bit_compute_dtype\": \"float32\",\n","    \"bnb_4bit_quant_storage\": \"uint8\",\n","    \"bnb_4bit_quant_type\": \"fp4\",\n","    \"bnb_4bit_use_double_quant\": false,\n","    \"llm_int8_enable_fp32_cpu_offload\": false,\n","    \"llm_int8_has_fp16_weight\": false,\n","    \"llm_int8_skip_modules\": null,\n","    \"llm_int8_threshold\": 6.0,\n","    \"load_in_4bit\": false,\n","    \"load_in_8bit\": true,\n","    \"quant_method\": \"bitsandbytes\"\n","  },\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.42.3\",\n","  \"use_cache\": false,\n","  \"vocab_size\": 128256\n","}\n","\n"]}],"source":["# # `config.json` cheking\n","# config = model.config\n","# print(config)"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T03:36:41.523488Z","iopub.status.busy":"2024-07-21T03:36:41.522664Z","iopub.status.idle":"2024-07-21T03:36:41.967331Z","shell.execute_reply":"2024-07-21T03:36:41.965963Z","shell.execute_reply.started":"2024-07-21T03:36:41.523456Z"},"trusted":true},"outputs":[],"source":["from transformers import Trainer\n","\n","output_dir = \"/kaggle/working/trained-model\"\n","\n","# save model\n","trainer.save_model(output_dir)"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T03:37:52.103634Z","iopub.status.busy":"2024-07-21T03:37:52.103245Z","iopub.status.idle":"2024-07-21T03:37:52.111679Z","shell.execute_reply":"2024-07-21T03:37:52.110256Z","shell.execute_reply.started":"2024-07-21T03:37:52.103602Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files in 'trained-model': ['config.json', 'special_tokens_map.json', 'adapter_config.json', 'adapter_model.safetensors', 'README.md', 'training_args.bin', 'tokenizer.json', 'tokenizer_config.json']\n"]}],"source":["# import os\n","# # Directory where the model is saved\n","# output_dir = \"trained-model\"\n","\n","# # List files in the directory\n","# files = os.listdir(output_dir)\n","# print(f\"Files in '{output_dir}': {files}\")"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T03:46:33.177482Z","iopub.status.busy":"2024-07-21T03:46:33.176858Z","iopub.status.idle":"2024-07-21T03:46:35.135602Z","shell.execute_reply":"2024-07-21T03:46:35.134545Z","shell.execute_reply.started":"2024-07-21T03:46:33.177452Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model  /kaggle/working/trained-model.zip saved.\n"]}],"source":["import shutil\n","\n","# Create a ZIP file\n","zip_file_path = \"/kaggle/working/trained-model.zip\"\n","shutil.make_archive(\"/kaggle/working/trained-model\", 'zip', output_dir)\n","\n","print(f\"Model saved to {zip_file_path}.\")\n"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2024-07-21T03:46:38.658148Z","iopub.status.busy":"2024-07-21T03:46:38.657448Z","iopub.status.idle":"2024-07-21T03:46:38.667510Z","shell.execute_reply":"2024-07-21T03:46:38.666220Z","shell.execute_reply.started":"2024-07-21T03:46:38.658115Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Output directory contents:\n","trained-model.zip\n","config.json\n","peft_lab_outputs\n","qa_pairs.json\n",".virtual_documents\n","wandb\n","experiments.zip\n","experiments\n","trained-model\n"]}],"source":["# import os\n","\n","# # List of files in the output directory\n","# print(\"Output directory contents:\")\n","# for filename in os.listdir('/kaggle/working'):\n","#     print(filename)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5420775,"sourceId":8998989,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}

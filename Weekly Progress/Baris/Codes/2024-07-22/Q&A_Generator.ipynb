{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import en_core_web_sm\n","import json\n","import numpy as np\n","import random\n","import re\n","import torch\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n","    AutoModelForSequenceClassification,\n",")\n","from typing import Any, List, Mapping, Tuple\n","\n","\n","class QuestionGenerator:\n","    \"\"\"A transformer-based NLP system for generating reading comprehension-style questions from\n","    texts. It can generate full sentence questions, multiple choice questions, or a mix of the\n","    two styles.\n","\n","    To filter out low quality questions, questions are assigned a score and ranked once they have\n","    been generated. Only the top k questions will be returned. This behaviour can be turned off\n","    by setting use_evaluator=False.\n","    \"\"\"\n","\n","    def __init__(self) -> None:\n","\n","        QG_PRETRAINED = \"iarfmoose/t5-base-question-generator\"\n","        self.ANSWER_TOKEN = \"<answer>\"\n","        self.CONTEXT_TOKEN = \"<context>\"\n","        self.SEQ_LENGTH = 512\n","\n","        self.device = torch.device(\n","            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        self.qg_tokenizer = AutoTokenizer.from_pretrained(\n","            QG_PRETRAINED, use_fast=False)\n","        self.qg_model = AutoModelForSeq2SeqLM.from_pretrained(QG_PRETRAINED)\n","        self.qg_model.to(self.device)\n","        self.qg_model.eval()\n","\n","        self.qa_evaluator = QAEvaluator()\n","\n","    def generate(\n","        self,\n","        article: str,\n","        use_evaluator: bool = True,\n","        num_questions: bool = None,\n","        answer_style: str = \"all\"\n","    ) -> List:\n","        \"\"\"Takes an article and generates a set of question and answer pairs. If use_evaluator\n","        is True then QA pairs will be ranked and filtered based on their quality. answer_style\n","        should selected from [\"all\", \"sentences\", \"multiple_choice\"].\n","        \"\"\"\n","\n","        # print(\"Generating questions...\\n\")\n","\n","        qg_inputs, qg_answers = self.generate_qg_inputs(article, answer_style)\n","        generated_questions = self.generate_questions_from_inputs(qg_inputs)\n","\n","        message = \"{} questions doesn't match {} answers\".format(\n","            len(generated_questions), len(qg_answers)\n","        )\n","        assert len(generated_questions) == len(qg_answers), message\n","\n","        if use_evaluator:\n","            # print(\"Evaluating QA pairs...\\n\")\n","            encoded_qa_pairs = self.qa_evaluator.encode_qa_pairs(\n","                generated_questions, qg_answers\n","            )\n","            scores = self.qa_evaluator.get_scores(encoded_qa_pairs)\n","\n","            if num_questions:\n","                qa_list = self._get_ranked_qa_pairs(\n","                    generated_questions, qg_answers, scores, num_questions\n","                )\n","            else:\n","                qa_list = self._get_ranked_qa_pairs(\n","                    generated_questions, qg_answers, scores\n","                )\n","\n","        else:\n","            # print(\"Skipping evaluation step.\\n\")\n","            qa_list = self._get_all_qa_pairs(generated_questions, qg_answers)\n","\n","        return qa_list\n","\n","    def generate_qg_inputs(self, text: str, answer_style: str) -> Tuple[List[str], List[str]]:\n","        \"\"\"Given a text, returns a list of model inputs and a list of corresponding answers.\n","        Model inputs take the form \"answer_token <answer text> context_token <context text>\" where\n","        the answer is a string extracted from the text, and the context is the wider text surrounding\n","        the context.\n","        \"\"\"\n","\n","        VALID_ANSWER_STYLES = [\"all\", \"sentences\", \"multiple_choice\"]\n","\n","        if answer_style not in VALID_ANSWER_STYLES:\n","            raise ValueError(\n","                \"Invalid answer style {}. Please choose from {}\".format(\n","                    answer_style, VALID_ANSWER_STYLES\n","                )\n","            )\n","\n","        inputs = []\n","        answers = []\n","\n","        if answer_style == \"sentences\" or answer_style == \"all\":\n","            segments = self._split_into_segments(text)\n","\n","            for segment in segments:\n","                sentences = self._split_text(segment)\n","                prepped_inputs, prepped_answers = self._prepare_qg_inputs(\n","                    sentences, segment\n","                )\n","                inputs.extend(prepped_inputs)\n","                answers.extend(prepped_answers)\n","\n","        if answer_style == \"multiple_choice\" or answer_style == \"all\":\n","            sentences = self._split_text(text)\n","            prepped_inputs, prepped_answers = self._prepare_qg_inputs_MC(\n","                sentences\n","            )\n","            inputs.extend(prepped_inputs)\n","            answers.extend(prepped_answers)\n","\n","        return inputs, answers\n","\n","    def generate_questions_from_inputs(self, qg_inputs: List) -> List[str]:\n","        \"\"\"Given a list of concatenated answers and contexts, with the form:\n","        \"answer_token <answer text> context_token <context text>\", generates a list of\n","        questions.\n","        \"\"\"\n","        generated_questions = []\n","\n","        for qg_input in qg_inputs:\n","            question = self._generate_question(qg_input)\n","            generated_questions.append(question)\n","\n","        return generated_questions\n","\n","    def _split_text(self, text: str) -> List[str]:\n","        \"\"\"Splits the text into sentences, and attempts to split or truncate long sentences.\"\"\"\n","        MAX_SENTENCE_LEN = 128\n","        sentences = re.findall(\".*?[.!\\?]\", text)\n","        cut_sentences = []\n","\n","        for sentence in sentences:\n","            if len(sentence) > MAX_SENTENCE_LEN:\n","                cut_sentences.extend(re.split(\"[,;:)]\", sentence))\n","\n","        # remove useless post-quote sentence fragments\n","        cut_sentences = [s for s in sentences if len(s.split(\" \")) > 5]\n","        sentences = sentences + cut_sentences\n","\n","        return list(set([s.strip(\" \") for s in sentences]))\n","\n","    def _split_into_segments(self, text: str) -> List[str]:\n","        \"\"\"Splits a long text into segments short enough to be input into the transformer network.\n","        Segments are used as context for question generation.\n","        \"\"\"\n","        MAX_TOKENS = 490\n","        paragraphs = text.split(\"\\n\")\n","        tokenized_paragraphs = [\n","            self.qg_tokenizer(p)[\"input_ids\"] for p in paragraphs if len(p) > 0\n","        ]\n","        segments = []\n","\n","        while len(tokenized_paragraphs) > 0:\n","            segment = []\n","\n","            while len(segment) < MAX_TOKENS and len(tokenized_paragraphs) > 0:\n","                paragraph = tokenized_paragraphs.pop(0)\n","                segment.extend(paragraph)\n","            segments.append(segment)\n","\n","        return [self.qg_tokenizer.decode(s, skip_special_tokens=True) for s in segments]\n","\n","    def _prepare_qg_inputs(\n","        self,\n","        sentences: List[str],\n","        text: str\n","    ) -> Tuple[List[str], List[str]]:\n","        \"\"\"Uses sentences as answers and the text as context. Returns a tuple of (model inputs, answers).\n","        Model inputs are \"answer_token <answer text> context_token <context text>\"\n","        \"\"\"\n","        inputs = []\n","        answers = []\n","\n","        for sentence in sentences:\n","            qg_input = f\"{self.ANSWER_TOKEN} {sentence} {self.CONTEXT_TOKEN} {text}\"\n","            inputs.append(qg_input)\n","            answers.append(sentence)\n","\n","        return inputs, answers\n","\n","    def _prepare_qg_inputs_MC(self, sentences: List[str]) -> Tuple[List[str], List[str]]:\n","        \"\"\"Performs NER on the text, and uses extracted entities are candidate answers for multiple-choice\n","        questions. Sentences are used as context, and entities as answers. Returns a tuple of (model inputs, answers).\n","        Model inputs are \"answer_token <answer text> context_token <context text>\"\n","        \"\"\"\n","        spacy_nlp = en_core_web_sm.load()\n","        docs = list(spacy_nlp.pipe(sentences, disable=[\"parser\"]))\n","        inputs_from_text = []\n","        answers_from_text = []\n","\n","        for doc, sentence in zip(docs, sentences):\n","            entities = doc.ents\n","            if entities:\n","\n","                for entity in entities:\n","                    qg_input = f\"{self.ANSWER_TOKEN} {entity} {self.CONTEXT_TOKEN} {sentence}\"\n","                    answers = self._get_MC_answers(entity, docs)\n","                    inputs_from_text.append(qg_input)\n","                    answers_from_text.append(answers)\n","\n","        return inputs_from_text, answers_from_text\n","\n","    def _get_MC_answers(self, correct_answer: Any, docs: Any) -> List[Mapping[str, Any]]:\n","        \"\"\"Finds a set of alternative answers for a multiple-choice question. Will attempt to find\n","        alternatives of the same entity type as correct_answer if possible.\n","        \"\"\"\n","        entities = []\n","\n","        for doc in docs:\n","            entities.extend([{\"text\": e.text, \"label_\": e.label_}\n","                            for e in doc.ents])\n","\n","        # remove duplicate elements\n","        entities_json = [json.dumps(kv) for kv in entities]\n","        pool = set(entities_json)\n","        num_choices = (\n","            min(4, len(pool)) - 1\n","        )  # -1 because we already have the correct answer\n","\n","        # add the correct answer\n","        final_choices = []\n","        correct_label = correct_answer.label_\n","        final_choices.append({\"answer\": correct_answer.text, \"correct\": True})\n","        pool.remove(\n","            json.dumps({\"text\": correct_answer.text,\n","                       \"label_\": correct_answer.label_})\n","        )\n","\n","        # find answers with the same NER label\n","        matches = [e for e in pool if correct_label in e]\n","\n","        # if we don't have enough then add some other random answers\n","        if len(matches) < num_choices:\n","            choices = matches\n","            pool = pool.difference(set(choices))\n","            choices.extend(random.sample(pool, num_choices - len(choices)))\n","        else:\n","            choices = random.sample(matches, num_choices)\n","\n","        choices = [json.loads(s) for s in choices]\n","\n","        for choice in choices:\n","            final_choices.append({\"answer\": choice[\"text\"], \"correct\": False})\n","\n","        random.shuffle(final_choices)\n","        return final_choices\n","\n","    @torch.no_grad()\n","    def _generate_question(self, qg_input: str) -> str:\n","        \"\"\"Takes qg_input which is the concatenated answer and context, and uses it to generate\n","        a question sentence. The generated question is decoded and then returned.\n","        \"\"\"\n","        encoded_input = self._encode_qg_input(qg_input)\n","        output = self.qg_model.generate(input_ids=encoded_input[\"input_ids\"])\n","        question = self.qg_tokenizer.decode(\n","            output[0],\n","            skip_special_tokens=True\n","        )\n","        return question\n","\n","    def _encode_qg_input(self, qg_input: str) -> torch.tensor:\n","        \"\"\"Tokenizes a string and returns a tensor of input ids corresponding to indices of tokens in\n","        the vocab.\n","        \"\"\"\n","        return self.qg_tokenizer(\n","            qg_input,\n","            padding='max_length',\n","            max_length=self.SEQ_LENGTH,\n","            truncation=True,\n","            return_tensors=\"pt\",\n","        ).to(self.device)\n","\n","    def _get_ranked_qa_pairs(\n","        self, generated_questions: List[str], qg_answers: List[str], scores, num_questions: int = 10\n","    ) -> List[Mapping[str, str]]:\n","        \"\"\"Ranks generated questions according to scores, and returns the top num_questions examples.\n","        \"\"\"\n","        if num_questions > len(scores):\n","            num_questions = len(scores)\n","            # print((\n","            #     f\"\\nWas only able to generate {num_questions} questions.\",\n","            #     \"For more questions, please input a longer text.\")\n","            # )\n","\n","        qa_list = []\n","\n","        for i in range(num_questions):\n","            index = scores[i]\n","            qa = {\n","                \"question\": generated_questions[index].split(\"?\")[0] + \"?\",\n","                \"answer\": qg_answers[index]\n","            }\n","            qa_list.append(qa)\n","\n","        return qa_list\n","\n","    def _get_all_qa_pairs(self, generated_questions: List[str], qg_answers: List[str]):\n","        \"\"\"Formats question and answer pairs without ranking or filtering.\"\"\"\n","        qa_list = []\n","\n","        for question, answer in zip(generated_questions, qg_answers):\n","            qa = {\n","                \"question\": question.split(\"?\")[0] + \"?\",\n","                \"answer\": answer\n","            }\n","            qa_list.append(qa)\n","\n","        return qa_list\n","\n","\n","class QAEvaluator:\n","    \"\"\"Wrapper for a transformer model which evaluates the quality of question-answer pairs.\n","    Given a QA pair, the model will generate a score. Scores can be used to rank and filter\n","    QA pairs.\n","    \"\"\"\n","\n","    def __init__(self) -> None:\n","\n","        QAE_PRETRAINED = \"iarfmoose/bert-base-cased-qa-evaluator\"\n","        self.SEQ_LENGTH = 512\n","\n","        self.device = torch.device(\n","            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        self.qae_tokenizer = AutoTokenizer.from_pretrained(QAE_PRETRAINED)\n","        self.qae_model = AutoModelForSequenceClassification.from_pretrained(\n","            QAE_PRETRAINED\n","        )\n","        self.qae_model.to(self.device)\n","        self.qae_model.eval()\n","\n","    def encode_qa_pairs(self, questions: List[str], answers: List[str]) -> List[torch.tensor]:\n","        \"\"\"Takes a list of questions and a list of answers and encodes them as a list of tensors.\"\"\"\n","        encoded_pairs = []\n","\n","        for question, answer in zip(questions, answers):\n","            encoded_qa = self._encode_qa(question, answer)\n","            encoded_pairs.append(encoded_qa.to(self.device))\n","\n","        return encoded_pairs\n","\n","    def get_scores(self, encoded_qa_pairs: List[torch.tensor]) -> List[float]:\n","        \"\"\"Generates scores for a list of encoded QA pairs.\"\"\"\n","        scores = {}\n","\n","        for i in range(len(encoded_qa_pairs)):\n","            scores[i] = self._evaluate_qa(encoded_qa_pairs[i])\n","\n","        return [\n","            k for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)\n","        ]\n","\n","    def _encode_qa(self, question: str, answer: str) -> torch.tensor:\n","        \"\"\"Concatenates a question and answer, and then tokenizes them. Returns a tensor of\n","        input ids corresponding to indices in the vocab.\n","        \"\"\"\n","        if type(answer) is list:\n","            for a in answer:\n","                if a[\"correct\"]:\n","                    correct_answer = a[\"answer\"]\n","        else:\n","            correct_answer = answer\n","\n","        return self.qae_tokenizer(\n","            text=question,\n","            text_pair=correct_answer,\n","            padding=\"max_length\",\n","            max_length=self.SEQ_LENGTH,\n","            truncation=True,\n","            return_tensors=\"pt\",\n","        )\n","\n","    @torch.no_grad()\n","    def _evaluate_qa(self, encoded_qa_pair: torch.tensor) -> float:\n","        \"\"\"Takes an encoded QA pair and returns a score.\"\"\"\n","        output = self.qae_model(**encoded_qa_pair)\n","        return output[0][0][1]\n","\n","\n","def print_qa(qa_list: List[Mapping[str, str]], show_answers: bool = True) -> None:\n","    \"\"\"Formats and prints a list of generated questions and answers.\"\"\"\n","\n","    for i in range(len(qa_list)):\n","        # wider space for 2 digit q nums\n","        space = \" \" * int(np.where(i < 9, 3, 4))\n","\n","        print(f\"{i + 1}) Q: {qa_list[i]['question']}\")\n","\n","        answer = qa_list[i][\"answer\"]\n","\n","        # print a list of multiple choice answers\n","        if type(answer) is list:\n","\n","            if show_answers:\n","                print(\n","                    f\"{space}A: 1. {answer[0]['answer']} \"\n","                    f\"{np.where(answer[0]['correct'], '(correct)', '')}\"\n","                )\n","                for j in range(1, len(answer)):\n","                    print(\n","                        f\"{space + '   '}{j + 1}. {answer[j]['answer']} \"\n","                        f\"{np.where(answer[j]['correct']==True,'(correct)', '')}\"\n","                    )\n","\n","            else:\n","                print(f\"{space}A: 1. {answer[0]['answer']}\")\n","                for j in range(1, len(answer)):\n","                    print(f\"{space + '   '}{j + 1}. {answer[j]['answer']}\")\n","\n","            print(\"\")\n","\n","        # print full sentence answers\n","        else:\n","            if show_answers:\n","                print(f\"{space}A: {answer}\\n\")"],"metadata":{"id":"Hw2JTSWXO24B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import zipfile\n","import os\n","import json\n","from typing import List\n","import time\n","import sys\n","\n","# Function to extract .txt files from a ZIP archive\n","def extract_txt_files(zip_path: str, extract_folder: str) -> List[str]:\n","    \"\"\"Extracts .txt files from a ZIP archive to a specified folder.\n","\n","    Args:\n","        zip_path (str): Path to the ZIP file.\n","        extract_folder (str): Folder where .txt files will be extracted.\n","\n","    Returns:\n","        List[str]: List of paths to the extracted .txt files.\n","    \"\"\"\n","    if not os.path.exists(extract_folder):\n","        os.makedirs(extract_folder)\n","\n","    txt_files = []\n","\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        for file_info in zip_ref.infolist():\n","            if file_info.filename.endswith('.txt'):\n","                zip_ref.extract(file_info, extract_folder)\n","                txt_files.append(os.path.join(extract_folder, file_info.filename))\n","\n","    return txt_files\n","\n","# Function to display progress bar\n","def print_progress_bar(iteration: int, total: int, start_time: float):\n","    \"\"\"Displays a progress bar in the console.\n","\n","    Args:\n","        iteration (int): Current iteration.\n","        total (int): Total number of iterations.\n","        start_time (float): Start time of the process.\n","    \"\"\"\n","    progress = (iteration / total)\n","    bar_length = 50\n","    block = int(round(bar_length * progress))\n","    progress_text = f\"\\r|{'█' * block}{'-' * (bar_length - block)}| {progress * 100:.1f}% Complete\"\n","\n","    elapsed_time = time.time() - start_time\n","    estimated_total_time = elapsed_time / progress if progress > 0 else 0\n","    remaining_time = estimated_total_time - elapsed_time\n","\n","    print(progress_text, end='')\n","    print(f\"  Elapsed time: {elapsed_time:.2f}s  Estimated remaining time: {remaining_time:.2f}s\", end='', flush=True)\n","\n","# Example usage\n","zip_path = '/content/articles.zip'\n","extract_folder = '/content/extracted_articles'\n","txt_file_paths = extract_txt_files(zip_path, extract_folder)\n","\n","# Initialize the QuestionGenerator\n","question_generator = QuestionGenerator()\n","\n","# List to store Q&A pairs\n","output_data = []\n","\n","# Limit for the number of questions\n","question_limit = 5000  # Adjust this limit as needed\n","total_questions_generated = 0\n","\n","# Initialize progress bar\n","total_files = len(txt_file_paths)\n","start_time = time.time()\n","\n","# Process each .txt file\n","for i, txt_file_path in enumerate(txt_file_paths):\n","    if total_questions_generated >= question_limit:\n","        break  # Stop processing if the limit is reached\n","\n","    with open(txt_file_path, 'r', encoding='utf-8') as file:\n","        article_text = file.read()\n","\n","    # Generate questions\n","    questions = question_generator.generate(article_text, use_evaluator=True, num_questions=5, answer_style='sentences')\n","\n","    # Save questions and answers in JSON format\n","    for q in questions:\n","        if total_questions_generated >= question_limit:\n","            break  # Stop adding questions if the limit is reached\n","\n","        output_data.append({\n","            \"question\": q['question'],\n","            \"answer\": q['answer']\n","        })\n","        total_questions_generated += 1\n","\n","    # Update progress bar\n","    print_progress_bar(i + 1, total_files, start_time)\n","\n","# Final progress bar update\n","print_progress_bar(total_files, total_files, start_time)\n","print()  # For new line after the progress bar\n","\n","# Save to JSON file\n","output_file = \"qa_pairs.json\"\n","with open(output_file, \"w\") as f:\n","    json.dump(output_data, f, indent=4)\n","print(f\"Output saved to {output_file}\")"],"metadata":{"id":"LHb-qDGjPEAl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721624725052,"user_tz":240,"elapsed":3480657,"user":{"displayName":"LLM","userId":"06425135503772392268"}},"outputId":"5b925ef7-becb-4c8d-826e-88b97bf4a7f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":["|██████████████████████████████████████████████----| 91.7% Complete  Elapsed time: 3004.79s  Estimated remaining time: 273.16s"]},{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["|██████████████████████████████████████████████████| 100.0% Complete  Elapsed time: 3477.76s  Estimated remaining time: 0.00s\n","Output saved to qa_pairs.json\n"]}]}]}
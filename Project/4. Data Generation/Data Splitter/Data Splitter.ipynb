{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pairs: 36744\n",
      "33069 pairs saved as train.json.\n",
      "3675 pairs saved as test.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON file (with utf-8 encoding)\n",
    "with open('36744_CNSC_QA_pairs.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Update the keys\n",
    "updated_data = [{'question': entry['prompt'], 'answer': entry['response']} for entry in data]\n",
    "\n",
    "# Find the total number of pairs\n",
    "total_pairs = len(updated_data)\n",
    "print(f\"Total number of pairs: {total_pairs}\")\n",
    "\n",
    "# Split into training and test sets\n",
    "train_size = int(total_pairs * 0.90)  # 85%\n",
    "test_size = total_pairs - train_size  # Remaining\n",
    "\n",
    "train_data = updated_data[:train_size]\n",
    "test_data = updated_data[train_size:]\n",
    "\n",
    "# Save the JSON files\n",
    "with open('train_.json', 'w', encoding='utf-8') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "with open('test_.json', 'w', encoding='utf-8') as test_file:\n",
    "    json.dump(test_data, test_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"{train_size} pairs saved as train.json.\")\n",
    "print(f\"{test_size} pairs saved as test.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pairs (limited): 10000\n",
      "9000 pairs saved as train_.json.\n",
      "1000 pairs saved as test_.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "MAX_PAIRS = 10000 \n",
    "\n",
    "# Read the JSON file (with utf-8 encoding)\n",
    "try:\n",
    "    with open('36744_CNSC_QA_pairs.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file '36744_CNSC_QA_pairs.json' was not found.\")\n",
    "    exit()\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Error: The file '36744_CNSC_QA_pairs.json' is not a valid JSON.\")\n",
    "    exit()\n",
    "\n",
    "# Update the keys\n",
    "updated_data = [{'question': entry.get('prompt', ''), 'answer': entry.get('response', '')} for entry in data]\n",
    "\n",
    "# Limit the total number of pairs\n",
    "limited_data = updated_data[:MAX_PAIRS]\n",
    "total_pairs = len(limited_data)\n",
    "print(f\"Total number of pairs (limited): {total_pairs}\")\n",
    "\n",
    "# Split into training and test sets\n",
    "train_size = int(total_pairs * 0.90)  # 90% for training\n",
    "test_size = total_pairs - train_size  # Remaining for testing\n",
    "\n",
    "train_data = limited_data[:train_size]\n",
    "test_data = limited_data[train_size:]\n",
    "\n",
    "# Save the JSON files\n",
    "try:\n",
    "    with open('train_.json', 'w', encoding='utf-8') as train_file:\n",
    "        json.dump(train_data, train_file, indent=4, ensure_ascii=False)\n",
    "    with open('test_.json', 'w', encoding='utf-8') as test_file:\n",
    "        json.dump(test_data, test_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"{train_size} pairs saved as train_.json.\")\n",
    "    print(f\"{test_size} pairs saved as test_.json.\")\n",
    "except IOError as e:\n",
    "    print(f\"Error saving files: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of question-answer pairs in train.json: 800\n",
      "Processed question-answer pairs saved as processed_train.json.\n",
      "Total number of question-answer pairs in test.json: 200\n",
      "Processed question-answer pairs saved as processed_test.json.\n"
     ]
    }
   ],
   "source": [
    "def process_answers(data):\n",
    "    processed_data = []\n",
    "    for entry in data:\n",
    "        if isinstance(entry['answer'], list):\n",
    "            # If 'answer' is a list of options, find the correct answer\n",
    "            correct_answer = next(item['answer'] for item in entry['answer'] if item['correct'])\n",
    "            processed_entry = {\n",
    "                'question': entry['question'],\n",
    "                'answer': correct_answer\n",
    "            }\n",
    "        else:\n",
    "            # If 'answer' is already a string, keep it as is\n",
    "            processed_entry = entry\n",
    "        processed_data.append(processed_entry)\n",
    "    return processed_data\n",
    "\n",
    "def process_file(file_path, output_path):\n",
    "    # Read the JSON file (with utf-8 encoding)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Find the total number of question-answer pairs\n",
    "    total_pairs = len(data)\n",
    "    print(f\"Total number of question-answer pairs in {file_path}: {total_pairs}\")\n",
    "\n",
    "    # Process the answers to ensure 'answer' contains only the correct answer\n",
    "    processed_data = process_answers(data)\n",
    "\n",
    "    # Save the processed data to a new JSON file\n",
    "    with open(output_path, 'w', encoding='utf-8') as processed_file:\n",
    "        json.dump(processed_data, processed_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Processed question-answer pairs saved as {output_path}.\")\n",
    "\n",
    "# Process both train.json and test.json\n",
    "process_file('train.json', 'processed_train.json')\n",
    "process_file('test.json', 'processed_test.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of question-answer pairs in train.json: 33869\n",
      "Combined data saved as train.json.\n",
      "Total number of question-answer pairs in test.json: 3875\n",
      "Combined data saved as test.json.\n"
     ]
    }
   ],
   "source": [
    "def merge_files(file_paths, output_path):\n",
    "    combined_data = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            combined_data.extend(data)\n",
    "    \n",
    "    # Print the total number of question-answer pairs\n",
    "    total_pairs = len(combined_data)\n",
    "    print(f\"Total number of question-answer pairs in {output_path}: {total_pairs}\")\n",
    "    \n",
    "    # Save the combined data to the output file\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(combined_data, output_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Combined data saved as {output_path}.\")\n",
    "\n",
    "# File paths for train data\n",
    "train_file_paths = ['train_.json', 'processed_train.json']\n",
    "# File paths for test data\n",
    "test_file_paths = ['test_.json', 'processed_test.json']\n",
    "\n",
    "# Merge the files and save the results\n",
    "merge_files(train_file_paths, 'train.json')\n",
    "merge_files(test_file_paths, 'test.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_file = '/kaggle/input/baris-fine-tuning/train.json'\n",
    "train_output_file = '/kaggle/working/train_dataset.json'\n",
    "\n",
    "test_input_file = '/kaggle/input/baris-fine-tuning/test.json'\n",
    "test_output_file = '/kaggle/working/test_dataset.json'\n",
    "\n",
    "def add_questions_key(input_file, output_file):\n",
    "    # Read the JSON file\n",
    "    with open(input_file, 'r') as infile:\n",
    "        data = json.load(infile)        \n",
    "    # Format the data by adding the 'questions' key\n",
    "    formatted_data = {\n",
    "        \"questions\": data\n",
    "    }\n",
    "\n",
    "    # Write the formatted data to a new file\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(formatted_data, outfile, indent=4)\n",
    "\n",
    "# Convert training and test datasets to the appropriate format\n",
    "add_questions_key(train_input_file, train_output_file)\n",
    "add_questions_key(test_input_file, test_output_file)\n",
    "\n",
    "print(\"JSON files have been formatted and saved successfully.\")\n",
    "\n",
    "with open(\"test_dataset.json\") as json_file:\n",
    "    test = json.load(json_file)    \n",
    "with open(\"train_dataset.json\") as json_file:\n",
    "    train = json.load(json_file)\n",
    "pd.DataFrame(train[\"questions\"]).head()\n",
    "pd.DataFrame(test[\"questions\"]).head()\n",
    "pprint(train[\"questions\"][0], sort_dicts=False)\n",
    "pprint(test[\"questions\"][0], sort_dicts=False)\n",
    "\n",
    "# Function to check data format\n",
    "def check_data_format(data):\n",
    "    if \"questions\" not in data or not isinstance(data[\"questions\"], list):\n",
    "        raise ValueError(\"The data does not contain the 'questions' key or it is not a list.\")\n",
    "\n",
    "check_data_format(train)\n",
    "check_data_format(test)\n",
    "\n",
    "# Define the prompt format\n",
    "prompt = \"\"\"Below is a question paired with an answer. Please write a response that appropriately completes the request.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Answer:\n",
    "{}\"\"\"\n",
    "\n",
    "# Get special tokens and EOS token from the tokenizer\n",
    "special_tokens = tokenizer.special_tokens_map_extended\n",
    "eos_token = tokenizer.eos_token\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Function to format prompts\n",
    "def formatting_prompts_func(examples):\n",
    "    questions = examples[\"question\"]\n",
    "    answers = examples[\"answer\"]\n",
    "    texts = []\n",
    "    for question, answer in zip(questions, answers):\n",
    "        # Format the text according to the prompt and append eos_token\n",
    "        text = prompt.format(question, answer) + eos_token\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Function to convert data into dataset format\n",
    "def create_and_format_dataset(data):\n",
    "    dataset_dict = {\n",
    "        \"question\": [item[\"question\"] for item in data[\"questions\"]],\n",
    "        \"answer\": [item[\"answer\"] for item in data[\"questions\"]],\n",
    "    }\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    # Apply the formatting prompts function and remove 'text' column\n",
    "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "    dataset = dataset.remove_columns([\"text\"])\n",
    "    return dataset\n",
    "\n",
    "# Create and format training and test datasets\n",
    "train_dataset = create_and_format_dataset(train)\n",
    "test_dataset = create_and_format_dataset(test)\n",
    "\n",
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "# Check the formatted dataset\n",
    "print(dataset_dict)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the input texts\n",
    "    inputs = tokenizer(examples['question'], padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt')\n",
    "    labels = tokenizer(examples['answer'], padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt')\n",
    "    \n",
    "    # Add labels to inputs\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    # Create attention masks for the inputs\n",
    "    inputs['attention_mask'] = inputs['attention_mask']\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "# print(train_dataset[0])\n",
    "# print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_file = '/kaggle/input/baris-fine-tuning/train.json'\n",
    "train_output_file = '/kaggle/working/train_dataset.json'\n",
    "test_input_file = '/kaggle/input/baris-fine-tuning/test.json'\n",
    "test_output_file = '/kaggle/working/test_dataset.json'\n",
    "\n",
    "def add_questions_key(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "    \n",
    "    formatted_data = {\n",
    "        \"questions\": data\n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(formatted_data, outfile, indent=4)\n",
    "\n",
    "add_questions_key(train_input_file, train_output_file)\n",
    "add_questions_key(test_input_file, test_output_file)\n",
    "\n",
    "print(\"JSON dosyaları formatlandı ve başarıyla kaydedildi.\")\n",
    "\n",
    "with open(train_output_file) as json_file:\n",
    "    train = json.load(json_file)\n",
    "with open(test_output_file) as json_file:\n",
    "    test = json.load(json_file)\n",
    "\n",
    "pd.DataFrame(train[\"questions\"]).head()\n",
    "pd.DataFrame(test[\"questions\"]).head()\n",
    "pprint(train[\"questions\"][0], sort_dicts=False)\n",
    "pprint(test[\"questions\"][0], sort_dicts=False)\n",
    "\n",
    "def check_data_format(data):\n",
    "    if \"questions\" not in data or not isinstance(data[\"questions\"], list):\n",
    "        raise ValueError(\"Veri 'questions' anahtarını içermiyor veya liste değil.\")\n",
    "\n",
    "check_data_format(train)\n",
    "check_data_format(test)\n",
    "\n",
    "prompt = \"\"\"Below is a question paired with an answer. Please write a response that appropriately completes the request.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Answer:\n",
    "{}\"\"\"\n",
    "\n",
    "special_tokens = tokenizer.special_tokens_map_extended\n",
    "eos_token = tokenizer.eos_token\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    questions = examples[\"question\"]\n",
    "    answers = examples[\"answer\"]\n",
    "    texts = []\n",
    "    for question, answer in zip(questions, answers):\n",
    "        text = prompt.format(question, answer) + eos_token\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "def create_and_format_dataset(data):\n",
    "    dataset_dict = {\n",
    "        \"question\": [item[\"question\"] for item in data[\"questions\"]],\n",
    "        \"answer\": [item[\"answer\"] for item in data[\"questions\"]],\n",
    "    }\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "    dataset = dataset.remove_columns([\"text\"])\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_and_format_dataset(train)\n",
    "test_dataset = create_and_format_dataset(test)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset  \n",
    "})\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['question'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "    labels = tokenizer(examples['answer'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "    \n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    inputs['attention_mask'] = inputs['attention_mask']\n",
    "    \n",
    "    return inputs\n",
    "dataset['train'] = dataset['train'].map(preprocess_function, batched=True)\n",
    "dataset['test'] = dataset['test'].map(preprocess_function, batched=True)\n",
    "\n",
    "print(dataset)\n",
    "# print(train_dataset[0])\n",
    "# print(train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

Scope

Evaluating data Generation methodologies for fine-tuning LLM
Summary of Project: The primary goal of the project is to explore and evaluate different data generation methodologies for fine-tuning a large language model (LLM). The aim is to determine which methodology yields the best performance based on specific evaluation metrics.
Objectives
•	Explore various data generation methodologies for the fine-tuning of LLMs.
•	Evaluate the fine-tuned LLMs using specific performance metrics.
•	Determine the best data generation methodologies for fine-tuning an LLM.
Scope
•	Research available open-source datasets and LLMs.
•	Develop code to run and fine-tune LLMs.
•	Generate data using different methodologies for fine-tuning.
•	Fine-tune the LLM using generated datasets.
•	Evaluate the performance of the fine-tuned LLMs.
Expected Outcomes
•	Working code for automatic data generation for fine-tuning an LLM.
•	Working code for running and fine-tuning an LLM.
•	Analysis and comparison of different data generation methodologies.
•	Fine-tuned LLM models with performance evaluations.
•	Documentation and reports summarizing the findings and methodologies.
Data
•	Use public datasets related to the nuclear industry, such as books, research papers, articles, journals, etc.
•	Ensure the datasets are free for commercial use.
Methodologies/ plan of attack

1)	Perform research on Data, and LLMs (1 weeks)
Tasks:
•	Identify available open-source datasets related to nuclear industry.
•	Investigate methods to collect the data efficiently.
•	Identify latest available open-source LLMs such as NuclearN.ai, LLAMA, Grok, etc.
•	Access the performances evaluation criteria of LLMs such as MMLU, TruthfulQA, etc while selecting an LLM. 
•	Assess the data format and requirements for fine-tuning LLMs.
•	Access the computational resources required for inferencing, fine-tuning, etc. 
•	Ensure data and models are free for commercial use. 
	Outcomes:
•	Quick progress update and feedback session. 
•	Selected dataset resources.
•	Selected a few suitable LLMs for the project.  
•	Identified the specific fine-tuning requirements for the chosen model.
•	A written summary of findings for every step.
2)	Gather data (0.5 week)
Tasks:
•	Collect the data using the identified methods.
Outcomes
•	Collected data with a good directory structure.
3)	Develop code to run Open-Source non-fine tuned LLM (0.5 week) 
Tasks: 
•	Obtain the model files from a trusted source or repository (e.g., Hugging Face, GitHub).  
•	Set-up virtual environment. 
•	Install necessary libraries and frameworks.  
•	Develop inference logic to generate responses from the model. 
Outcomes: 
•	An inference ready LLM integrated into your environment, capable of processing and responding to user inputs.  
4)	Data Generation for fine-tuning LLM (3 weeks)
Tasks:
•	Research and identify different methodologies for generating fine-tuning data from the selected dataset.
•	Ensure that the data generation methodologies align with the requirements of the chosen LLM.
•	Develop code to implement data generation methodologies.
	Outcomes:
•	Script(s) that can be used to generate data for fine tuning LLM utilizing the selected methodologies.
•	Multiple directories, one for every selected data generation methodology, containing data in proper format for fine-tuning LLM.
5)	Fine-tuning LLM (2 weeks)
Tasks:
•	Research and identify different techniques to fine-tune an LLM.
•	Fine-tune the chosen LLM model on each of the different datasets.
•	Document the process, challenges, and solutions encountered during fine-tuning.
	Outcomes:
•	Fine-tuned models, one for every selected data methodology.
•	Detailed documentation of the fine-tuning process and challenges.
6)	Model Evaluation (1 week)
Tasks:
•	Identify and select appropriate metrics for evaluating the performance of the fine-tuned LLM models.
•	Evaluate each fine-tuned model based on the selected metrics.
•	Perform a comparative analysis of the models fine-tuned with different data generation methodologies.
•	Analyze the evaluation results to determine which data generation methodology produced the best performance.
•	Offer a detailed explanation and justification for the chosen methodology based on the evaluation metrics.
•	Suggest potential improvements or future work.
	Outcomes:
•	Evaluation reports for each fine-tuned model.
•	Comparative analysis report highlighting the strengths and weaknesses of each data generation methodology.
•	Recommendations for the best data generation methodology based on the evaluation metrics.

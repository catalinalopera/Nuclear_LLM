{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5786bd3",
   "metadata": {},
   "source": [
    "## Generation QA pairs for LLM fine-tuning using OpenAI API\n",
    "\n",
    "### Overview\n",
    "This script automates the extraction of text from PDF files, splits the text into manageable chunks, generates question-answer (QA) pairs using OpenAI language model, and saves the results to a JSON file. The JSON format is specifically structured for fine-tuning large language models (LLMs) with the generated QA pairs. The script utilizes the OpenAI API, which requires a paid API key for access.\n",
    "\n",
    "### Configuration\n",
    "The script begins by setting up necessary configurations, including the source directory for the PDF files, parameters for text chunking, and the OpenAI model settings. It also sets the OpenAI API key required for accessing the language model. This API key must be a valid, paid key to use the OpenAI services.\n",
    "\n",
    "### Functions\n",
    "*Extract Text from PDFs*\n",
    "This function reads all PDF files in the specified directory and extracts their text content. It handles errors, ensuring that the script continues to run even if some files cannot be processed. The extracted text from each PDF is stored in a list.\n",
    "\n",
    "*Chunk Text*\n",
    "This function splits the extracted text into smaller chunks with a specified overlap. The chunking process makes it easier for the language model to process the text, as smaller chunks are more manageable. The function takes in parameters for chunk size and overlap, allowing flexibility in how the text is split.\n",
    "\n",
    "*Generate QA Pairs*\n",
    "This function generates QA pairs from the text chunks using the OpenAI language model. It runs the model on each chunk of text and parses the generated output to extract questions and answers. The function also limits the number of QA pairs generated per chunk to the specified maximum, ensuring that the output is not overly verbose and remains relevant. Since this function relies on the OpenAI API, it requires a paid API key to function.\n",
    "\n",
    "*Save QA Pairs to JSON*\n",
    "This function cleans up the QA pairs by removing any special tags (e.g., <question>, </question>, <answer>, </answer>) and saves the cleaned pairs to a JSON file. The JSON format is required for fine-tuning LLMs with the generated QA pairs. Each QA pair is saved as a dictionary with \"prompt\" and \"response\" keys.\n",
    "\n",
    "*Main Execution*\n",
    "The main execution block orchestrates the entire process. It first extracts text from all PDF files in the specified directory. Then, it splits the extracted text into chunks. For each chunk, the script generates QA pairs and collects them. Finally, it saves all generated QA pairs to a JSON file. Throughout the process, the script prints debugging information to the console, including the number of files processed, chunks created, QA pairs generated, and the final count of QA pairs saved.\n",
    "\n",
    "### Summary of Key Features\n",
    "Text Extraction: The script extracts text from PDF files in the specified directory, handling errors to ensure robustness.\n",
    "\n",
    "Text Chunking: It splits the extracted text into smaller, manageable chunks based on specified chunk size and overlap parameters.\n",
    "\n",
    "QA Pair Generation: The script generates QA pairs using the OpenAI language model, which requires a paid API key. It ensures relevance by limiting the number of QA pairs per chunk.\n",
    "\n",
    "JSON Saving: Generated QA pairs are cleaned and saved in a JSON format suitable for fine-tuning large language models (LLMs).\n",
    "\n",
    "Error Handling: The script includes basic error handling to manage issues with PDF file processing, ensuring continuity of the overall process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a6f62",
   "metadata": {},
   "source": [
    "### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai langchain pandas tqdm PyMuPDF\n",
    "# pip install -U langchain-community\n",
    "# pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# Load configuration\n",
    "SOURCE_DIRECTORY = '/Users/zarinadossayeva/Desktop/WIL_LLM/CNSC_QA_pairs_JSON/CNSC_docs_1_10'\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 100\n",
    "model = \"gpt-3.5-turbo-0125\"\n",
    "temperature = 0\n",
    "max_tokens = None\n",
    "max_qa_per_chunk = 5  # Limiting the number of QA pairs per chunk\n",
    "\n",
    "# Set OpenAI API key\n",
    "api_key = 'open-api-key'\n",
    "os.environ['OPENAI_API_KEY'] = api_key\n",
    "\n",
    "# Verify API key is set\n",
    "print(f\"Using OpenAI API key: {os.environ.get('OPENAI_API_KEY')}\")  # Debugging line\n",
    "\n",
    "# Define a function to read all PDFs and extract text\n",
    "def extract_text_from_pdfs(directory):\n",
    "    \"\"\"Extracts text from all PDF files in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The directory containing PDF files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings, each containing the extracted text from a PDF file.\n",
    "    \"\"\"\n",
    "    text_data = []\n",
    "    pdf_files = glob.glob(os.path.join(directory, \"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")  # Debugging line\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(\"No PDF files found in the specified directory.\")  # Debugging line\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing file: {pdf_file}\")  # Debugging line\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(file_path=pdf_file)\n",
    "            documents = loader.load()  # Expecting a list of documents\n",
    "            for document in documents:\n",
    "                text = document.page_content\n",
    "                text_data.append(text)\n",
    "                print(f\"Extracted text from {pdf_file} (length {len(text)}): {text[:500]}...\")  # Debugging line\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {pdf_file}: {e}\")  # Debugging line\n",
    "\n",
    "    return text_data\n",
    "\n",
    "# Convert documents to chunks\n",
    "def chunk_text(text, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"Splits the text into smaller chunks with a specified overlap.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be split.\n",
    "        chunk_size (int, optional): The size of each chunk. Defaults to 1000.\n",
    "        chunk_overlap (int, optional): The overlap between chunks. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of text chunks.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(f\"Chunked text into {len(chunks)} chunks (total length {len(text)}):\")  # Debugging line\n",
    "    if chunks:\n",
    "        print(f\"First chunk: {chunks[0]}\")  # Debugging line\n",
    "    return chunks\n",
    "\n",
    "# Prompt to use OpenAI API as QA generator\n",
    "qa_prompt_template = \"\"\"\n",
    "You are an intelligent and supportive assistant.\n",
    "Your task is to create question-answer pairs from the given context.\n",
    "Question type: Context based/Yes-No/ short question answer/ long question answer.\n",
    "Just use the information in the context to write question and answer.\n",
    "Please don't make up anything outside the given context.\n",
    "\n",
    "Text: {context}\n",
    "\n",
    "Generate as many question-answer pairs as possible. Always use tags to enclose question answers as shown in below examples.\n",
    "\n",
    "<question>What is SMR?</question>\n",
    "<answer>SMR stands for Small Modular Reactors, which are smaller, more flexible nuclear energy plants that can be deployed in various settings, including large established grids, smaller grids, remote off grid communities, and resource projects. They are designed to provide non-emitting baseload generation and can support intermittent renewable sources like wind and solar. They are also capable of producing steam for industrial purposes.</answer>\n",
    "<question>What are the key objectives of the SMR project at the Darlington site in Ontario?</question>\n",
    "<answer>The key objectives of the SMR project at the Darlington site in Ontario are to maintain a diverse generation supply mix to minimize carbon emissions from electricity generation in the province, to demonstrate a First-Of-A-Kind (FOAK) SMR to be ready for deployment across Canada by 2030, and to ensure economic development by securing Canadian content both for domestic and export projects from the developer in exchange for providing the opportunity to deploy their FOAK unit and be a first mover towards an SMR fleet.</answer>\n",
    "... (continue as needed)\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the LangChain prompt\n",
    "qa_prompt = PromptTemplate(template=qa_prompt_template, input_variables=[\"context\"])\n",
    "llm = ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)\n",
    "\n",
    "# Create an LLMChain\n",
    "qa_chain = LLMChain(prompt=qa_prompt, llm=llm)\n",
    "\n",
    "# Generate QA pairs for every chunk\n",
    "def generate_qa_pairs(chunks, max_qa_per_chunk):\n",
    "    \"\"\"Generates question-answer pairs from text chunks using the language model.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): The list of text chunks.\n",
    "        max_qa_per_chunk (int): The maximum number of QA pairs to generate per chunk.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing a prompt and response.\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=\"Generating QA pairs\")):\n",
    "        try:\n",
    "            print(f\"Processing chunk {i + 1}/{len(chunks)} (length {len(chunk)}): {chunk[:500]}...\")  # Debugging line\n",
    "            response = qa_chain.run({\"context\": chunk})\n",
    "            print(f\"Generated QA pairs for chunk {i + 1}/{len(chunks)}: {response}\")  # Debugging line\n",
    "            \n",
    "            # Parse the QA pairs from the response\n",
    "            qa_list = response.split('<question>')\n",
    "            for qa in qa_list[1:max_qa_per_chunk+1]:\n",
    "                if '<answer>' in qa:\n",
    "                    question = \"<question>\" + qa.split('</question>')[0] + \"</question>\"\n",
    "                    answer = \"<answer>\" + qa.split('<answer>')[1].split('</answer>')[0] + \"</answer>\"\n",
    "                    qa_pairs.append({\"prompt\": question, \"response\": answer})\n",
    "                else:\n",
    "                    qa_pairs.append({\"prompt\": qa.strip(), \"response\": \"\"})\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating QA for chunk {i + 1}/{len(chunks)}: {e}\")\n",
    "    return qa_pairs\n",
    "\n",
    "# Save questions and answers to a JSON file\n",
    "def save_questions_to_json(qa_pairs, output_file=\"CNSC_QA_pairs_1_10.json\"):\n",
    "    \"\"\"Saves the generated QA pairs to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (list): The list of QA pairs to save.\n",
    "        output_file (str, optional): The name of the output JSON file. Defaults to \"questions.json\".\n",
    "    \"\"\"\n",
    "    cleaned_qa_pairs = []\n",
    "    for qa in qa_pairs:\n",
    "        prompt = qa['prompt'].replace(\"<question>\", \"\").replace(\"</question>\", \"\").strip()\n",
    "        response = qa['response'].replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "        cleaned_qa_pairs.append({\"prompt\": prompt, \"response\": response})\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cleaned_qa_pairs, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Saved {len(cleaned_qa_pairs)} QA pairs to {output_file}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Main execution function to run the entire pipeline: \n",
    "    extract text, chunk text, generate QA pairs, and save to JSON.\n",
    "    \"\"\"\n",
    "    text_data = extract_text_from_pdfs(SOURCE_DIRECTORY)\n",
    "    all_qa_pairs = []\n",
    "    \n",
    "    for text in text_data:\n",
    "        chunks = chunk_text(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        qa_pairs = generate_qa_pairs(chunks, max_qa_per_chunk)\n",
    "        all_qa_pairs.extend(qa_pairs)\n",
    "    \n",
    "    print(f\"Total QA pairs generated: {len(all_qa_pairs)}\")  # Printing total QA pairs generated\n",
    "    save_questions_to_json(all_qa_pairs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

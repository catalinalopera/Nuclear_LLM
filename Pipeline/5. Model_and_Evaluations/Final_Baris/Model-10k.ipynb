{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9121525,"sourceType":"datasetVersion","datasetId":5506328}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nassert device == torch.device('cuda'), \"Not using CUDA. Set: Runtime > Change runtime type > Hardware Accelerator: GPU\"","metadata":{"execution":{"iopub.status.busy":"2024-08-07T03:39:58.838933Z","iopub.execute_input":"2024-08-07T03:39:58.839200Z","iopub.status.idle":"2024-08-07T03:40:03.111977Z","shell.execute_reply.started":"2024-08-07T03:39:58.839177Z","shell.execute_reply":"2024-08-07T03:40:03.111008Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%%capture\nimport torch\n!pip install bitsandbytes\n!pip install datasets\nmajor_version, minor_version = torch.cuda.get_device_capability()\nif major_version >= 8:\n    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\nelse:\n    !pip install --no-deps xformers trl peft accelerate bitsandbytes\npass","metadata":{"execution":{"iopub.status.busy":"2024-08-07T03:41:06.578974Z","iopub.execute_input":"2024-08-07T03:41:06.579820Z","iopub.status.idle":"2024-08-07T03:41:40.735731Z","shell.execute_reply.started":"2024-08-07T03:41:06.579770Z","shell.execute_reply":"2024-08-07T03:41:40.734661Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch, os, json, random, bitsandbytes as bnb, torch.nn as nn, psutil\nfrom datasets import Dataset, DatasetDict, load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, TrainingArguments, BitsAndBytesConfig\nfrom trl import SFTTrainer\nimport re, gc\nfrom pprint import pprint\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-08-07T03:41:46.203691Z","iopub.execute_input":"2024-08-07T03:41:46.204737Z","iopub.status.idle":"2024-08-07T03:42:02.120423Z","shell.execute_reply.started":"2024-08-07T03:41:46.204690Z","shell.execute_reply":"2024-08-07T03:42:02.119593Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-08-07 03:41:51.326507: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-07 03:41:51.326636: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-07 03:41:51.480604: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Loader","metadata":{}},{"cell_type":"code","source":"HF_TOKEN = \"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\"\nmodel_name = \"meta-llama/Meta-Llama-3-8B\"\n\ndef load_model_and_tokenizer():\n    \"\"\"Load the model and tokenizer with configurations.\"\"\"\n    try:\n        print(\"Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=HF_TOKEN)\n        tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.padding_side = \"right\"\n\n        special_tokens = tokenizer.special_tokens_map_extended\n        eos_token = tokenizer.eos_token\n        eos_token_id = tokenizer.eos_token_id\n\n        print(\"EOS Token:\", eos_token)\n        print(\"EOS Token ID:\", eos_token_id)\n        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n        print(\"Loading model...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=quantization_config,\n            device_map='auto',\n            low_cpu_mem_usage=True,\n            use_auth_token=HF_TOKEN\n        )\n        print(\"Model and tokenizer loaded and configured successfully.\")\n        return model, tokenizer\n    except Exception as e:\n        print(\"An error occurred:\", e)\n        return None, None\n\ndef apply_lora_config(model):\n    \"\"\"Apply LoRA configuration to the model.\"\"\"\n    try:\n        print(\"Applying LoRA configuration...\")\n        lora_config = LoraConfig(\n            r=18,\n            lora_alpha=8,\n            target_modules=[\"q_proj\", \"v_proj\"],\n            lora_dropout=0.1,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\"\n        )\n        model = get_peft_model(model, lora_config)\n        print(\"LoRA configuration applied successfully.\")\n        return model\n    except Exception as e:\n        print(\"An error occurred while applying LoRA configuration:\", e)\n        return model\nmodel, tokenizer = load_model_and_tokenizer()\n\nif model and tokenizer:\n    model = apply_lora_config(model)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T03:42:05.646338Z","iopub.execute_input":"2024-08-07T03:42:05.647022Z","iopub.status.idle":"2024-08-07T03:43:54.927074Z","shell.execute_reply.started":"2024-08-07T03:42:05.646993Z","shell.execute_reply":"2024-08-07T03:43:54.926110Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Loading tokenizer...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:778: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"220e9323c33e4dc6b0879870e2065770"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40855e96048447f39e149f35e795febb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa0755d4c7e14f0ea600c34823249877"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"EOS Token: <|end_of_text|>\nEOS Token ID: 128001\nLoading model...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"332abefce81f4947913fab985da86b05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba8b17624f3b4d8ab0d777967046a7df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b550d41e587b41898f7847b781b1775c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d235bc4e0fe24d3a9ffcf6e657d28bac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b2f513fe75d4b38ac65d02c4133d518"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a642a3810ec4bcaa5972466af2026f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd6e02cd5a9c4b4697a1646468ee3175"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4a6a868460a4a35b0c72a1a098f2b2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dec2372497af4f98a853ce8dc7419f0a"}},"metadata":{}},{"name":"stdout","text":"Model and tokenizer loaded and configured successfully.\nApplying LoRA configuration...\nLoRA configuration applied successfully.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Dataset Converting to Appropriate Format for Huggingface Transformers","metadata":{}},{"cell_type":"code","source":"# Define file paths\ntrain_input_file = '/kaggle/input/baris-fine-tuning-10k/train.json'\ntrain_output_file = '/kaggle/working/train_dataset.json'\ntest_input_file = '/kaggle/input/baris-fine-tuning-10k/test.json'\ntest_output_file = '/kaggle/working/test_dataset.json'\n\ndef add_questions_key(input_file, output_file):\n    # Read JSON file\n    with open(input_file, 'r') as infile:\n        data = json.load(infile)\n\n    # Format data with 'questions' key\n    formatted_data = {\n        \"questions\": data\n    }\n\n    # Write formatted data to a new file\n    with open(output_file, 'w') as outfile:\n        json.dump(formatted_data, outfile, indent=4)\n\n# Convert training and test datasets to the appropriate format\nadd_questions_key(train_input_file, train_output_file)\nadd_questions_key(test_input_file, test_output_file)\n\nprint(\"JSON files have been formatted and saved successfully.\")\n\nwith open(train_output_file) as json_file:\n    train = json.load(json_file)\nwith open(test_output_file) as json_file:\n    test = json.load(json_file)\n\n# Convert data to DataFrame and check\npd.DataFrame(train[\"questions\"]).head()\npd.DataFrame(test[\"questions\"]).head()\npprint(train[\"questions\"][0], sort_dicts=False)\npprint(test[\"questions\"][0], sort_dicts=False)\n\n# Function to check data format\ndef check_data_format(data):\n    if \"questions\" not in data or not isinstance(data[\"questions\"], list):\n        raise ValueError(\"Data does not contain 'questions' key or it is not a list.\")\n\ncheck_data_format(train)\ncheck_data_format(test)\n\n# Define the CoT prompt format\ncot_prompt = \"\"\"Below is a question with a potential answer. Please follow the steps to determine the most accurate and comprehensive answer.\n\n### Question:\n{}\n\n### Possible Answer:\n{}\n\n### Thought Process:\n1. Evaluate the potential answer based on the following criteria:\n   - **Accuracy:** Is the information provided in the answer correct and factually accurate?\n   - **Completeness:** Does the answer cover all necessary aspects and details of the question?\n   - **Relevance:** Is the answer directly related to the question and its context?\n   - **Clarity:** Is the answer presented clearly and understandably?\n2. Determine the most accurate and comprehensive answer.\n\n### Final Answer:\n{}\"\"\"\n\n# Get special tokens and EOS token from tokenizer\nspecial_tokens = tokenizer.special_tokens_map_extended\neos_token = tokenizer.eos_token\neos_token_id = tokenizer.eos_token_id\n\ndef formatting_prompts_func(examples):\n    questions = examples[\"question\"]\n    answers = examples[\"answer\"]\n    \n    formatted_texts = []\n    for question, answer in zip(questions, answers):\n        # Prompt'u formatlamak için yer tutucuların sayısını kontrol edin\n        text = cot_prompt.format(question, answer, answer) + eos_token\n        formatted_texts.append(text)\n    \n    return {\"text\": formatted_texts}\n\ndef create_and_format_dataset(data):\n    dataset_dict = {\n        \"question\": [item[\"question\"] for item in data[\"questions\"]],\n        \"answer\": [item[\"answer\"] for item in data[\"questions\"]],\n    }\n    dataset = Dataset.from_dict(dataset_dict)\n    \n    dataset = dataset.map(formatting_prompts_func, batched=True)\n    return dataset\n\ndef preprocess_function(examples):\n    inputs = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n    labels = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n\n    inputs['labels'] = labels['input_ids']\n    inputs['attention_mask'] = inputs['attention_mask']\n\n    return inputs\n\n# Create and format training and test datasets\ntrain_dataset = create_and_format_dataset(train)\ntest_dataset = create_and_format_dataset(test)\n\n# Create DatasetDict\ndataset = DatasetDict({\n    'train': train_dataset,\n    'test': test_dataset\n})\n\n# Apply preprocessing\ndataset['train'] = dataset['train'].map(preprocess_function, batched=True)\ndataset['test'] = dataset['test'].map(preprocess_function, batched=True)\n\nprint(dataset)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T03:45:53.103301Z","iopub.execute_input":"2024-08-07T03:45:53.103670Z","iopub.status.idle":"2024-08-07T03:46:01.129356Z","shell.execute_reply.started":"2024-08-07T03:45:53.103627Z","shell.execute_reply":"2024-08-07T03:46:01.128457Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"JSON files have been formatted and saved successfully.\n{'question': 'What is REGDOC-3.5.3, Version 3?',\n 'answer': 'REGDOC-3.5.3, Version 3 is a regulatory document that outlines the '\n           'CNSC processes and practices related to nuclear energy regulation. '\n           'It provides guidance on regulatory fundamentals for the nuclear '\n           'industry.'}\n{'question': 'Who is responsible for ensuring compliance with the requirements '\n             'set out in REGDOC-3.3.1?',\n 'answer': 'Applicants and licensees are responsible for ensuring compliance '\n           'with the requirements set out in REGDOC-3.3.1.'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"251a1629388f41baad50c0bc50a68cdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeea05e0bd6d43a59b223ea8e7bea6b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea1e0850206143be8036f5771930dfa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aae65c80d1174982b6be3e804c4a3354"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['question', 'answer', 'text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['question', 'answer', 'text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 1000\n    })\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom transformers import (\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling\n)\n\n# Login to Hugging Face\nlogin(token=\"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\")\ntraining_args = TrainingArguments(\n    output_dir=\"./kaggle/working/output\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    num_train_epochs=1,\n    fp16 = not torch.cuda.is_bf16_supported(),\n    logging_steps=150,\n    logging_dir=\"./kaggle/working/logs\",\n    optim=\"adamw_8bit\",\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.05,\n    weight_decay=0.01,\n    report_to=\"tensorboard\",\n    seed = 3407,\n    save_strategy=\"epoch\",\n    eval_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    save_total_limit=1,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['test'],\n    tokenizer=tokenizer\n)\n\n# Model\ntrainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T03:46:20.503839Z","iopub.execute_input":"2024-08-07T03:46:20.504204Z","iopub.status.idle":"2024-08-07T06:46:12.077735Z","shell.execute_reply.started":"2024-08-07T03:46:20.504178Z","shell.execute_reply":"2024-08-07T06:46:12.076808Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2250/2250 2:59:44, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.634000</td>\n      <td>0.668465</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nprint(eval_results)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:46:24.925682Z","iopub.execute_input":"2024-08-07T06:46:24.926446Z","iopub.status.idle":"2024-08-07T06:48:00.373507Z","shell.execute_reply.started":"2024-08-07T06:46:24.926413Z","shell.execute_reply":"2024-08-07T06:48:00.372569Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 01:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.6684646010398865, 'eval_runtime': 95.4354, 'eval_samples_per_second': 10.478, 'eval_steps_per_second': 1.31, 'epoch': 1.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"output_dir = \"./kaggle/working/output\"\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:48:14.440713Z","iopub.execute_input":"2024-08-07T06:48:14.441467Z","iopub.status.idle":"2024-08-07T06:48:14.896986Z","shell.execute_reply.started":"2024-08-07T06:48:14.441432Z","shell.execute_reply":"2024-08-07T06:48:14.896025Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"('./kaggle/working/output/tokenizer_config.json',\n './kaggle/working/output/special_tokens_map.json',\n './kaggle/working/output/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"HF_TOKEN = \"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\"\nlogin(token=HF_TOKEN)\nmodel.push_to_hub(\"777Nuclear/CoT2\", use_auth_token=HF_TOKEN)\ntokenizer.push_to_hub(\"777Nuclear/CoT2\", use_auth_token=HF_TOKEN)\n\nprint(\"Model and tokenizer have been pushed to Hugging Face Hub successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:48:20.405051Z","iopub.execute_input":"2024-08-07T06:48:20.405911Z","iopub.status.idle":"2024-08-07T06:48:26.625521Z","shell.execute_reply.started":"2024-08-07T06:48:20.405879Z","shell.execute_reply":"2024-08-07T06:48:26.624533Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:875: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/30.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cbca3eb1d9f4bf99fb1b8ba8703263e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2792bc5a0b9f4423aad11c8f232cbf85"}},"metadata":{}},{"name":"stdout","text":"Model and tokenizer have been pushed to Hugging Face Hub successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"import zipfile\nimport os\n\n# Paths\noutput_dir = \"./kaggle/working/output\"\nzip_path = \"./kaggle/working/output.zip\"\n\n# Function to zip the directory\ndef zip_directory(directory_path, zip_path):\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(directory_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                # Adding files to the zip file\n                zipf.write(file_path, os.path.relpath(file_path, directory_path))\n\n# Zip the output directory\nzip_directory(output_dir, zip_path)\nprint(f\"Model and tokenizer have been zipped to {zip_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:54:27.521463Z","iopub.execute_input":"2024-08-07T06:54:27.522155Z","iopub.status.idle":"2024-08-07T06:54:32.658395Z","shell.execute_reply.started":"2024-08-07T06:54:27.522118Z","shell.execute_reply":"2024-08-07T06:54:32.657377Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model and tokenizer have been zipped to ./kaggle/working/output.zip\n","output_type":"stream"}]}]}
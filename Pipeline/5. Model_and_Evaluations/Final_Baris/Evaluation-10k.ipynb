{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-07T07:16:38.102046Z","iopub.status.busy":"2024-08-07T07:16:38.101694Z","iopub.status.idle":"2024-08-07T07:16:38.107595Z","shell.execute_reply":"2024-08-07T07:16:38.106158Z","shell.execute_reply.started":"2024-08-07T07:16:38.102017Z"},"trusted":true},"outputs":[],"source":["import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","assert device == torch.device('cuda'), \"Not using CUDA. Set: Runtime > Change runtime type > Hardware Accelerator: GPU\""]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T07:16:39.661422Z","iopub.status.busy":"2024-08-07T07:16:39.660593Z","iopub.status.idle":"2024-08-07T07:17:15.562428Z","shell.execute_reply":"2024-08-07T07:17:15.561156Z","shell.execute_reply.started":"2024-08-07T07:16:39.661388Z"},"trusted":true},"outputs":[],"source":["%%capture\n","import torch\n","!pip install bitsandbytes\n","!pip install datasets\n","major_version, minor_version = torch.cuda.get_device_capability()\n","if major_version >= 8:\n","    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n","else:\n","    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n","pass"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T07:17:32.351190Z","iopub.status.busy":"2024-08-07T07:17:32.350329Z","iopub.status.idle":"2024-08-07T07:17:46.881402Z","shell.execute_reply":"2024-08-07T07:17:46.880637Z","shell.execute_reply.started":"2024-08-07T07:17:32.351152Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-08-07 07:17:37.143640: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-08-07 07:17:37.143746: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-08-07 07:17:37.277147: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import torch, os, json, random, bitsandbytes as bnb, torch.nn as nn, psutil\n","from datasets import Dataset, DatasetDict, load_dataset\n","from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, TrainingArguments, BitsAndBytesConfig\n","from trl import SFTTrainer\n","import re, gc\n","from pprint import pprint\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\n","import pandas as pd"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T07:18:00.649965Z","iopub.status.busy":"2024-08-07T07:18:00.648545Z","iopub.status.idle":"2024-08-07T07:20:39.094996Z","shell.execute_reply":"2024-08-07T07:20:39.094010Z","shell.execute_reply.started":"2024-08-07T07:18:00.649920Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading tokenizer...\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:778: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57330710a4eb4e6c93a365af9b690885","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4423ff1dc0a4ac993f1ff494402971a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f978d43a4a1d4d80b9473fba6fef79d4","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/335 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["EOS Token: <|end_of_text|>\n","EOS Token ID: 128001\n","Loading model...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f87b02b9a6a44a0299650749e4cc3f64","version_major":2,"version_minor":0},"text/plain":["adapter_config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"77ef140f0ff849b8bdc60e224b6c039b","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bde557bf210c4510822bdfe331e48ced","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d1048a691834f80aa44e93d6155cd97","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9bcf7f0eedf4ef68f4f59d8da20dd88","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18d43794c2ba4f6c9af8749252830e31","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95700cee26d64774a83e4b02a1d9edf8","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5eaaee69a00f4e8dbd8f56601be9600b","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eda1a1589442448ab58fbf7a57263a40","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8bb3ff2cb124b26931ed303f1ef3067","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"474e2912bd424931ac80e65aa3f11edd","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/30.7M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Model and tokenizer loaded and configured successfully.\n"]}],"source":["HF_TOKEN = \"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\"\n","model_name = \"777Nuclear/CoT2\"\n","\n","def load_model_and_tokenizer():\n","    \"\"\"Load the model and tokenizer with configurations.\"\"\"\n","    try:\n","        print(\"Loading tokenizer...\")\n","        tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=HF_TOKEN)\n","        tokenizer.pad_token = tokenizer.eos_token\n","        tokenizer.padding_side = \"right\"\n","\n","        special_tokens = tokenizer.special_tokens_map_extended\n","        eos_token = tokenizer.eos_token\n","        eos_token_id = tokenizer.eos_token_id\n","\n","        print(\"EOS Token:\", eos_token)\n","        print(\"EOS Token ID:\", eos_token_id)\n","        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n","        print(\"Loading model...\")\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_name,\n","            quantization_config=quantization_config,\n","            device_map='auto',\n","            low_cpu_mem_usage=True,\n","            use_auth_token=HF_TOKEN\n","        )\n","        print(\"Model and tokenizer loaded and configured successfully.\")\n","        return model, tokenizer\n","    except Exception as e:\n","        print(\"An error occurred:\", e)\n","        return None, None\n","\n","\n","model, tokenizer = load_model_and_tokenizer()\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T07:22:26.015707Z","iopub.status.busy":"2024-08-07T07:22:26.015037Z","iopub.status.idle":"2024-08-07T07:22:26.223568Z","shell.execute_reply":"2024-08-07T07:22:26.222675Z","shell.execute_reply.started":"2024-08-07T07:22:26.015674Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Applying LoRA configuration...\n","LoRA configuration applied successfully.\n"]}],"source":["def apply_lora_config(model):\n","    \"\"\"Apply LoRA configuration to the model.\"\"\"\n","    try:\n","        print(\"Applying LoRA configuration...\")\n","        lora_config = LoraConfig(\n","            r=18,\n","            lora_alpha=8,\n","            target_modules=[\"q_proj\", \"v_proj\"],\n","            lora_dropout=0.1,\n","            bias=\"none\",\n","            task_type=\"CAUSAL_LM\"\n","        )\n","        model = get_peft_model(model, lora_config)\n","        print(\"LoRA configuration applied successfully.\")\n","        return model\n","    except Exception as e:\n","        print(\"An error occurred while applying LoRA configuration:\", e)\n","        return model\n","if model and tokenizer:\n","    model = apply_lora_config(model)"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T07:54:55.409981Z","iopub.status.busy":"2024-08-07T07:54:55.409086Z","iopub.status.idle":"2024-08-07T07:54:57.798387Z","shell.execute_reply":"2024-08-07T07:54:57.797353Z","shell.execute_reply.started":"2024-08-07T07:54:55.409946Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BartTokenizer, BartForConditionalGeneration, pipeline\n","import torch\n","\n","tokenizer_summarizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n","model_summarizer = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n","summarizer = pipeline(\"summarization\", model=model_summarizer, tokenizer=tokenizer_summarizer, device=0)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T07:39:45.955688Z","iopub.status.busy":"2024-08-07T07:39:45.955282Z","iopub.status.idle":"2024-08-07T07:39:51.097393Z","shell.execute_reply":"2024-08-07T07:39:51.096639Z","shell.execute_reply.started":"2024-08-07T07:39:45.955654Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c86f25628c8f492cb62b75d448c24397","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/511 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6205976ed4440faae8e1a0cab628fb5","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a67cb03d9f545f3aea319284eee96c7","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d913cb1314249538462a63296b590c6","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"537159ba30d8411b850f127c39a9db17","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Text classification pipeline\n","evaluator = pipeline(\"text-classification\", model=\"textattack/bert-base-uncased-imdb\", device=0)\n","\n","def generate_responses(question: str, num_responses: int = 3) -> list:\n","    prompt = f\"Question: {question}\\nAnswer:\"\n","    encoding = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n","\n","    generation_config = {\n","        \"num_return_sequences\": num_responses,\n","        \"do_sample\": True,\n","        \"top_k\": 50,\n","        \"top_p\": 0.9,\n","        \"temperature\": 0.7,\n","        \"pad_token_id\": tokenizer.eos_token_id,\n","        \"eos_token_id\": tokenizer.eos_token_id,\n","        \"max_new_tokens\": 150\n","    }\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids=encoding.input_ids,\n","            attention_mask=encoding.attention_mask,\n","            **generation_config\n","        )\n","\n","    responses = [tokenizer.decode(output, skip_special_tokens=True).strip() for output in outputs]\n","    return responses\n","\n","def evaluate_relevance_and_coherence(response: str, question: str) -> tuple:\n","    relevance_score = evaluator(f\"{question} {response}\")[0]['score']\n","    coherence_score = evaluator(response)[0]['score']\n","    return relevance_score, coherence_score\n","\n","def evaluate_accuracy(response: str, question: str) -> float:\n","    # Simple keyword matching (more complex accuracy metrics can be used)\n","    keywords = [\"CNSC\"]  # Example keywords\n","    return sum(word in response for word in keywords) / len(keywords)\n","\n","def evaluate_completeness(response: str, question: str) -> float:\n","    # Evaluating the length of the response (example metric)\n","    response_length = len(response.split())\n","    question_length = len(question.split())\n","    return min(response_length / question_length, 1.0)  # Completeness score (1.0 is the highest score)\n","\n","def evaluate_clarity(response: str) -> float:\n","    # Evaluating the clarity of the response (example metric, more complex NLP techniques can be used)\n","    words = response.split()\n","    return max(0.0, 1.0 - (len(words) / 50))  # Shorter responses receive higher scores\n","\n","def evaluate_response(response: str, question: str) -> dict:\n","    relevance, coherence = evaluate_relevance_and_coherence(response, question)\n","    accuracy = evaluate_accuracy(response, question)\n","    completeness = evaluate_completeness(response, question)\n","    clarity = evaluate_clarity(response)\n","    \n","    return {\n","        \"relevance\": relevance,\n","        \"coherence\": coherence,\n","        \"accuracy\": accuracy,\n","        \"completeness\": completeness,\n","        \"clarity\": clarity\n","    }\n","\n","def select_best_response(responses: list, question: str) -> str:\n","    best_score = -float('inf')\n","    best_response = \"\"\n","    \n","    for response in responses:\n","        scores = evaluate_response(response, question)\n","        total_score = scores[\"relevance\"] + scores[\"coherence\"] + scores[\"accuracy\"] + scores[\"completeness\"] + scores[\"clarity\"]\n","        \n","        if total_score > best_score:\n","            best_score = total_score\n","            best_response = response\n","    \n","    return best_response if best_response else \"No suitable response found\"\n","\n","def summarize_text(text: str) -> str:\n","    summary = summarizer(text, max_length=150, min_length=30, do_sample=False)\n","    return summary[0]['summary_text']\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T07:40:06.444122Z","iopub.status.busy":"2024-08-07T07:40:06.443433Z","iopub.status.idle":"2024-08-07T07:40:46.578943Z","shell.execute_reply":"2024-08-07T07:40:46.578035Z","shell.execute_reply.started":"2024-08-07T07:40:06.444088Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Response: Question: What is the function of the CNSC?\n","Answer: The Central Nervous System Committee is a standing committee of the American Academy of Otolaryngology-Head and Neck Surgery, and is responsible for all issues related to the neurosciences. It is composed of 12 members, who are appointed for a three year term, and the chairperson, who is appointed for a one year term. The committee meets twice a year at the Academy’s Annual Meeting and at Midwinter Meeting. The committee’s activities are divided into four subcommittees: Basic Science, Education, Practice, and Research.\n","The committee’s activities are divided into four subcommittees: Basic Science, Education, Practice, and Research.\n","The Basic Science Subcommittee is responsible for the organization of the Academy’s annual meeting Basic Science\n","Summarized Response: The Central Nervous System Committee is a standing committee of the American Academy of Otolaryngology-Head and Neck Surgery. It is composed of 12 members, who are appointed for a three year term. The committee meets twice a year at the Academy’s Annual Meeting and at Midwinter Meeting.\n"]}],"source":["# Example usage \n","question = \"What is the function of the CNSC?\"\n","responses = generate_responses(question, num_responses=3)\n","best_response = select_best_response(responses, question)\n","summarized_response = summarize_text(best_response)\n","\n","print(\"Best Response:\", best_response)\n","print(\"Summarized Response:\", summarized_response)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T07:41:41.302019Z","iopub.status.busy":"2024-08-07T07:41:41.301663Z","iopub.status.idle":"2024-08-07T07:42:20.159512Z","shell.execute_reply":"2024-08-07T07:42:20.158585Z","shell.execute_reply.started":"2024-08-07T07:41:41.301990Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","Your max_length is set to 150, but your input_length is only 63. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n"]},{"name":"stdout","output_type":"stream","text":["Best Response: Question: What may modifications to a currently licensed facility require?\n","Answer: Any modification to a currently licensed facility that has a direct bearing on the safety of the patient or the quality of the care rendered must be approved by the licensing authority. A new application must be submitted and approved by the licensing authority.\n","Summarized Response: Any modification to a currently licensed facility that has a direct bearing on the safety of the patient or the quality of the care rendered must be approved. A new application must be submitted and approved by the licensing authority.\n"]}],"source":["question = \"What may modifications to a currently licensed facility require?\"\n","responses = generate_responses(question, num_responses=3)\n","best_response = select_best_response(responses, question)\n","summarized_response = summarize_text(best_response)\n","\n","print(\"Best Response:\", best_response)\n","print(\"Summarized Response:\", summarized_response)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T07:58:55.044318Z","iopub.status.busy":"2024-08-07T07:58:55.043480Z","iopub.status.idle":"2024-08-07T07:58:55.709348Z","shell.execute_reply":"2024-08-07T07:58:55.708300Z","shell.execute_reply.started":"2024-08-07T07:58:55.044283Z"},"trusted":true},"outputs":[],"source":["# Text classification pipeline\n","evaluator = pipeline(\"text-classification\", model=\"textattack/bert-base-uncased-imdb\", device=0)\n","\n","def generate_responses(question: str, num_responses: int = 3) -> list:\n","    prompt = f\"Question: {question}\\nAnswer:\"\n","    encoding = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n","\n","    generation_config = {\n","        \"num_return_sequences\": num_responses,\n","        \"do_sample\": True,\n","        \"top_k\": 50,\n","        \"top_p\": 0.9,\n","        \"temperature\": 0.7,\n","        \"pad_token_id\": tokenizer.eos_token_id,\n","        \"eos_token_id\": tokenizer.eos_token_id,\n","        \"max_new_tokens\": 150\n","    }\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids=encoding.input_ids,\n","            attention_mask=encoding.attention_mask,\n","            **generation_config\n","        )\n","\n","    responses = [tokenizer.decode(output, skip_special_tokens=True).strip() for output in outputs]\n","    return responses\n","\n","def evaluate_relevance_and_coherence(response: str, question: str) -> tuple:\n","    relevance_score = evaluator(f\"{question} {response}\")[0]['score']\n","    coherence_score = evaluator(response)[0]['score']\n","    return relevance_score, coherence_score\n","\n","def evaluate_accuracy(response: str, question: str) -> float:\n","    keywords = [\"CNSC\"]\n","    return sum(word in response for word in keywords) / len(keywords)\n","\n","def evaluate_completeness(response: str, question: str) -> float:\n","    response_length = len(response.split())\n","    question_length = len(question.split())\n","    return min(response_length / question_length, 1.0)\n","\n","def evaluate_clarity(response: str) -> float:\n","    words = response.split()\n","    return max(0.0, 1.0 - (len(words) / 50))\n","\n","def evaluate_response(response: str, question: str) -> dict:\n","    relevance, coherence = evaluate_relevance_and_coherence(response, question)\n","    accuracy = evaluate_accuracy(response, question)\n","    completeness = evaluate_completeness(response, question)\n","    clarity = evaluate_clarity(response)\n","    \n","    return {\n","        \"relevance\": relevance,\n","        \"coherence\": coherence,\n","        \"accuracy\": accuracy,\n","        \"completeness\": completeness,\n","        \"clarity\": clarity\n","    }\n","\n","def select_best_response(responses: list, question: str) -> str:\n","    best_score = -float('inf')\n","    best_response = \"\"\n","    \n","    for response in responses:\n","        scores = evaluate_response(response, question)\n","        total_score = scores[\"relevance\"] + scores[\"coherence\"] + scores[\"accuracy\"] + scores[\"completeness\"] + scores[\"clarity\"]\n","        \n","        if total_score > best_score:\n","            best_score = total_score\n","            best_response = response\n","    \n","    return best_response if best_response else \"No suitable response found\"\n","\n","def summarize_text(text: str) -> str:\n","    max_length = 1024\n","    min_length = 30\n","    summary_length = 150\n","\n","    if len(text) > max_length:\n","        chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n","        summaries = [summarizer(chunk, max_length=summary_length, min_length=min_length, do_sample=False)[0]['summary_text'] for chunk in chunks]\n","        full_summary = \" \".join(summaries)\n","        final_summary = summarizer(full_summary, max_length=summary_length, min_length=min_length, do_sample=False)\n","        return final_summary[0]['summary_text'].strip()\n","    else:\n","        summary = summarizer(text, max_length=summary_length, min_length=min_length, do_sample=False)\n","        return summary[0]['summary_text'].strip()\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T07:58:58.974121Z","iopub.status.busy":"2024-08-07T07:58:58.973226Z","iopub.status.idle":"2024-08-07T07:59:37.584792Z","shell.execute_reply":"2024-08-07T07:59:37.583799Z","shell.execute_reply.started":"2024-08-07T07:58:58.974074Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Your max_length is set to 150, but your input_length is only 114. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=57)\n"]},{"name":"stdout","output_type":"stream","text":["Best Response: Question: What is the function of the CNSC?\n","Answer: The Central Nervous System Committee is a subcommittee of the ICHS (International Council of Harmonization of Technical Requirements for Registration of Pharmaceuticals for Human Use). The CNSC is responsible for harmonizing the clinical investigation of drugs intended for use in the central nervous system (CNS) in the US, Europe, and Japan. The CNSC was created in 1993 as a result of the need to harmonize the requirements for the clinical investigation of drugs intended for use in the CNS.\n","Summarized Response: The Central Nervous System Committee is a subcommittee of the ICHS. The CNSC is responsible for harmonizing the clinical investigation of drugs intended for use in the central nervous system (CNS) in the US, Europe, and Japan.\n"]}],"source":["# Test\n","question = \"What is the function of the CNSC?\"\n","responses = generate_responses(question, num_responses=3)\n","best_response = select_best_response(responses, question)\n","summarized_response = summarize_text(best_response)\n","\n","print(\"Best Response:\", best_response)\n","print(\"Summarized Response:\", summarized_response)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T08:06:13.126019Z","iopub.status.busy":"2024-08-07T08:06:13.125349Z","iopub.status.idle":"2024-08-07T08:06:52.138016Z","shell.execute_reply":"2024-08-07T08:06:52.137070Z","shell.execute_reply.started":"2024-08-07T08:06:13.125987Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Your max_length is set to 150, but your input_length is only 60. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n"]},{"name":"stdout","output_type":"stream","text":["Best Response: Question: What is the function of the CNSC?\n","Answer: The Council is responsible for the setting and maintenance of standards of education, training and professional conduct for dental hygienists in Canada. The Council is also responsible for the registration of dental hygienists in Canada.\n","Summarized Response: The CNSC is responsible for the setting and maintenance of standards of education, training and professional conduct for dental hygienists in Canada.\n"]}],"source":["# Example usage\n","question = \"What is the function of the CNSC?\"\n","best_response = get_best_response_for_question(question)\n","summarized_response = summarize_text(best_response)\n","\n","print(\"Best Response:\", best_response)\n","print(\"Summarized Response:\", summarized_response)"]},{"cell_type":"markdown","metadata":{},"source":["# No Summarization"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T08:41:06.074926Z","iopub.status.busy":"2024-08-07T08:41:06.074103Z","iopub.status.idle":"2024-08-07T08:41:06.720296Z","shell.execute_reply":"2024-08-07T08:41:06.719340Z","shell.execute_reply.started":"2024-08-07T08:41:06.074892Z"},"trusted":true},"outputs":[],"source":["# Text classification pipeline for evaluating relevance and coherence\n","evaluator = pipeline(\"text-classification\", model=\"textattack/bert-base-uncased-imdb\", device=0)\n","\n","# Dictionary to cache responses\n","response_cache = {}\n","\n","def generate_responses(question: str, num_responses: int = 3) -> list:\n","    prompt = f\"Question: {question}\\nAnswer:\"\n","    encoding = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n","\n","    generation_config = {\n","        \"num_return_sequences\": num_responses,\n","        \"do_sample\": True,\n","        \"top_k\": 50,\n","        \"top_p\": 0.9,\n","        \"temperature\": 0.7,\n","        \"pad_token_id\": tokenizer.eos_token_id,\n","        \"eos_token_id\": tokenizer.eos_token_id,\n","        \"max_new_tokens\": 150\n","    }\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids=encoding.input_ids,\n","            attention_mask=encoding.attention_mask,\n","            **generation_config\n","        )\n","\n","    responses = [tokenizer.decode(output, skip_special_tokens=True).strip() for output in outputs]\n","    return responses\n","\n","def clean_response(response: str) -> str:\n","    # Remove parts of the response that start with \"Questions:\"\n","    if \"Questions:\" in response:\n","        parts = response.split(\"Questions:\", 1)\n","        return parts[0].strip()\n","    return response\n","\n","def evaluate_relevance_and_coherence(response: str, question: str) -> tuple:\n","    relevance_score = evaluator(f\"{question} {response}\")[0]['score']\n","    coherence_score = evaluator(response)[0]['score']\n","    return relevance_score, coherence_score\n","\n","def evaluate_accuracy(response: str, question: str) -> float:\n","    keywords = [word.lower() for word in question.split()]\n","    return sum(word.lower() in response.lower() for word in keywords) / len(keywords)\n","\n","def evaluate_completeness(response: str, question: str) -> float:\n","    response_length = len(response.split())\n","    question_length = len(question.split())\n","    return min(response_length / question_length, 1.0)\n","\n","def evaluate_clarity(response: str) -> float:\n","    words = response.split()\n","    return max(0.0, 1.0 - (len(words) / 50))\n","\n","def evaluate_response(response: str, question: str) -> dict:\n","    relevance, coherence = evaluate_relevance_and_coherence(response, question)\n","    accuracy = evaluate_accuracy(response, question)\n","    completeness = evaluate_completeness(response, question)\n","    clarity = evaluate_clarity(response)\n","    \n","    return {\n","        \"relevance\": relevance,\n","        \"coherence\": coherence,\n","        \"accuracy\": accuracy,\n","        \"completeness\": completeness,\n","        \"clarity\": clarity\n","    }\n","\n","def select_best_response(responses: list, question: str) -> str:\n","    best_score = -float('inf')\n","    best_response = \"\"\n","    \n","    for response in responses:\n","        # Clean response to remove extra questions and answers\n","        cleaned_response = clean_response(response)\n","        \n","        scores = evaluate_response(cleaned_response, question)\n","        total_score = scores[\"relevance\"] + scores[\"coherence\"] + scores[\"accuracy\"] + scores[\"completeness\"] + scores[\"clarity\"]\n","        \n","        if total_score > best_score:\n","            best_score = total_score\n","            best_response = cleaned_response\n","    \n","    return best_response if best_response else \"No suitable response found\"\n","\n","def get_best_response_for_question(question: str) -> str:\n","    # Check if the response is already cached\n","    if question in response_cache:\n","        return response_cache[question]\n","\n","    # Generate responses and select the best one\n","    responses = generate_responses(question, num_responses=3)\n","    best_response = select_best_response(responses, question)\n","\n","    # Cache the best response\n","    response_cache[question] = best_response\n","    return best_response"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T08:42:05.693006Z","iopub.status.busy":"2024-08-07T08:42:05.692653Z","iopub.status.idle":"2024-08-07T08:42:05.698328Z","shell.execute_reply":"2024-08-07T08:42:05.697456Z","shell.execute_reply.started":"2024-08-07T08:42:05.692978Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Response: Question: What is the purpose of the cost estimate in decommissioning planning?\n","Answer: The cost estimate is an important component of the decommissioning planning process. The cost estimate should be an accurate reflection of the costs associated with decommissioning the facility and the waste disposal requirements. The cost estimate should be prepared as part of the decommissioning plan to assist the licensee in determining the funding requirements for decommissioning and the time frame for the decommissioning activities. The cost estimate should also be used to evaluate the licensee's decommissioning plan to ensure that the licensee has adequately addressed the requirements of 10 CFR 20.1402.\n"]}],"source":["# Example usage\n","question = \"What is the purpose of the cost estimate in decommissioning planning?\"\n","best_response = get_best_response_for_question(question)\n","print(\"Best Response:\", best_response)"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T08:43:48.398817Z","iopub.status.busy":"2024-08-07T08:43:48.398023Z","iopub.status.idle":"2024-08-07T08:43:48.403503Z","shell.execute_reply":"2024-08-07T08:43:48.402542Z","shell.execute_reply.started":"2024-08-07T08:43:48.398782Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Response: Question: What is the function of the CNSC?\n","Answer: The CNSC monitors the progress of the clinical trials in the area of neuro-oncology and neurosurgery. It also develops guidelines for the treatment of brain tumours and brain metastases. It also monitors the outcome of patients with brain tumours. It also provides guidelines for the treatment of neurosurgical conditions.\n"]}],"source":["# Example usage\n","question = \"What is the function of the CNSC?\"\n","best_response = get_best_response_for_question(question)\n","print(\"Best Response:\", best_response)"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T08:45:12.684152Z","iopub.status.busy":"2024-08-07T08:45:12.683802Z","iopub.status.idle":"2024-08-07T08:45:51.026427Z","shell.execute_reply":"2024-08-07T08:45:51.025362Z","shell.execute_reply.started":"2024-08-07T08:45:12.684123Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Response: Question: What may modifications to a currently licensed facility require?\n","Answer: Modifications to a currently licensed facility that change the character of the facility or result in a new facility may require a new license. A facility may not be licensed for any use other than that for which it was licensed. If a facility changes its character or purpose, a new license must be obtained.\n"]}],"source":["# Example usage\n","question = \"What may modifications to a currently licensed facility require?\"\n","best_response = get_best_response_for_question(question)\n","print(\"Best Response:\", best_response)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}

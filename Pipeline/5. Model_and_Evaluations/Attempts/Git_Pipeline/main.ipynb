{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. GitHub Clone","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/Falgun1/NLP-Corpus\n%cd NLP-Corpus/Pipeline","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:56:22.636509Z","iopub.execute_input":"2024-07-24T15:56:22.637284Z","iopub.status.idle":"2024-07-24T15:56:24.492296Z","shell.execute_reply.started":"2024-07-24T15:56:22.637252Z","shell.execute_reply":"2024-07-24T15:56:24.491202Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'NLP-Corpus'...\nremote: Enumerating objects: 1421, done.\u001b[K\nremote: Counting objects: 100% (520/520), done.\u001b[K\nremote: Compressing objects: 100% (309/309), done.\u001b[K\nremote: Total 1421 (delta 417), reused 284 (delta 211), pack-reused 901\u001b[K\nReceiving objects: 100% (1421/1421), 2.88 MiB | 27.85 MiB/s, done.\nResolving deltas: 100% (693/693), done.\n/kaggle/working/NLP-Corpus/Pipeline\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. Library","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nassert device == torch.device('cuda'), \"Not using CUDA. Set: Runtime > Change runtime type > Hardware Accelerator: GPU\"","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:56:31.280489Z","iopub.execute_input":"2024-07-24T15:56:31.280857Z","iopub.status.idle":"2024-07-24T15:56:34.672580Z","shell.execute_reply.started":"2024-07-24T15:56:31.280824Z","shell.execute_reply":"2024-07-24T15:56:34.671642Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install -q bitsandbytes\n!pip install -q transformers\n!pip install -q nltk\n!pip install -q datasets\n!pip install -q textstat\n!pip install -q rouge_score\nmajor_version, minor_version = torch.cuda.get_device_capability()\nif major_version >= 8:\n    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\nelse:\n    !pip install --no-deps xformers trl peft accelerate bitsandbytes\npass","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:58:03.655971Z","iopub.execute_input":"2024-07-24T15:58:03.656844Z","iopub.status.idle":"2024-07-24T15:59:26.895609Z","shell.execute_reply.started":"2024-07-24T15:58:03.656811Z","shell.execute_reply":"2024-07-24T15:59:26.894288Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch,os, json, re, random  \nimport bitsandbytes as bnb\nimport torch.nn as nn\nimport pandas as pd\nfrom pprint import pprint\nfrom datasets import Dataset, DatasetDict, load_dataset\nfrom transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments, DataCollatorForLanguageModeling)\nfrom peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\nfrom huggingface_hub import login\nfrom trl import SFTTrainer\nfrom keywords_manager import KeywordsManager\nfrom wiki import WikiArticleFetcher, FilteredWikiArticleFetcher\nfrom file_utils import ZipExtractor\nfrom generator import QuestionGenerator, print_qa\nfrom question_generator import QuestionAnswerGenerator","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:59:34.225163Z","iopub.execute_input":"2024-07-24T15:59:34.225596Z","iopub.status.idle":"2024-07-24T15:59:36.834970Z","shell.execute_reply.started":"2024-07-24T15:59:34.225567Z","shell.execute_reply":"2024-07-24T15:59:36.833979Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# 3.Web Scraping","metadata":{}},{"cell_type":"code","source":"def data_collector():\n    wscraping = FilteredWikiArticleFetcher(keywords_manager=KeywordsManager(),file_limit=5,filtered_names = ['wiki_CNSC'] )\n    wscraping.fetch_and_save_articles()  \nif __name__ == \"__main__\":\n    data_collector()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:59:40.747105Z","iopub.execute_input":"2024-07-24T15:59:40.748422Z","iopub.status.idle":"2024-07-24T16:01:23.708225Z","shell.execute_reply.started":"2024-07-24T15:59:40.748382Z","shell.execute_reply":"2024-07-24T16:01:23.707301Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Articles found: 5\nArticle limit reached. Stopping the process.\n\nTotal articles found and added to ZIP: 5\nTotal time taken: 102.96 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"def zip_extractor():\n    extractor = ZipExtractor(zip_path = 'filtered_articles.zip', extract_to = 'Articles')\n    extractor.extract()\nif __name__ == \"__main__\":\n    zip_extractor()  \ndef list_files_in_directory(directory):\n    if not os.path.exists(directory):\n        print(f\"The directory {directory} does not exist.\")\n        return []\n    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n    return files\ndef print_files():\n    directory = 'Articles'\n    files = list_files_in_directory(directory)\n    if files:\n        print(f\"Files in '{directory}' directory:\")\n        for file in files:\n            print(file)\n    else:\n        print(\"No files found.\")\nif __name__ == \"__main__\":\n    print_files()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:01:32.371187Z","iopub.execute_input":"2024-07-24T16:01:32.371830Z","iopub.status.idle":"2024-07-24T16:01:32.382749Z","shell.execute_reply.started":"2024-07-24T16:01:32.371796Z","shell.execute_reply":"2024-07-24T16:01:32.381736Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Extracted filtered_articles.zip to Articles\nFiles in 'Articles' directory:\nwiki_CNSC_Bruce_Nuclear_Generating_Station.txt\nwiki_CNSC_Chalk_River_Laboratories.txt\nwiki_CNSC_National_Research_Universal_reactor.txt\nwiki_CNSC_Whiteshell_Laboratories.txt\nwiki_CNSC_Canadian_Nuclear_Safety_Commission.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4.Q&A Generator","metadata":{}},{"cell_type":"code","source":"def main():  \n    qag = QuestionAnswerGenerator(articles_folder = \"Articles\" , num_questions = 20, answer_style = 'all')\n    qag.generate_questions()\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:01:40.586834Z","iopub.execute_input":"2024-07-24T16:01:40.587496Z","iopub.status.idle":"2024-07-24T16:14:31.165741Z","shell.execute_reply.started":"2024-07-24T16:01:40.587463Z","shell.execute_reply":"2024-07-24T16:14:31.164717Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14135644d39e4754a9d05fc8855899b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbbaf0ba503e481a84cde2d80979fd42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e86705cbed7b4c3aa5c5c78dd520524e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a96fd86e420451685d913e1b1a4158a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/121 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d08f903bee54c888d0fa1e73c4757ed"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40b41972b3d0408fb362a413ca2f5b2b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18affcac2e984b3e99f142125c592de2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78f72fa0c98f44feb6702997f6970c5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"687f8f61f35c420b9ba36e614c58b9d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1ab0aac4cd6491a8498eaea38b29e0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"660ce9f2ad8b4c7c8f4aedf00f81f20e"}},"metadata":{}},{"name":"stdout","text":"Generating questions...\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Evaluating QA pairs...\n\n1) Q: What was the amount of water released from the NRU reactor?\n   A: In its formal report to the CNSC, filed on December 9, 2008 (when the volume of leakage was determined to meet the requirement for such a report) AECL mentioned that 47 litres (10 imp gal; 12 US gal) of heavy water were released from the reactor, about 10% of which evaporated and the rest contained, but affirmed that the spill was not serious and did not present a threat to public health.\n\n2) Q: What was the first time that a medical isotope was produced in nature?\n   A: With the construction of the earlier NRX reactor, it was possible for the first time to commercially manufacture isotopes that were not commonly found in nature.\n\n3) Q: How many workers were exposed to radiation during the refurbishment?\n   A: In January 2010, up to 217 workers were potentially exposed to radiation during the refurbishment.\n\n4) Q: What was the government's response to the shutdown of the NRU reactor?\n   A: On December 11, 2007, the House of Commons of Canada, acting on what the government described as \"independent expert\" advice, passed emergency legislation authorizing the restarting of the NRU reactor with one of the two seismic connections complete (one pump being sufficient to cool the core), and authorizing the reactor's operation for 120 days without CNSC approval.\n\n5) Q: What is the most common way for a developed country to support a national facility for neutr?\n   A: It is common for a developed country to support a national facility for neutron scattering and one for X-ray scattering.\n\n6) Q: What is the average price of electricity for the three units that were refurbished?\n   A: During the course of the refurbishment of Units 3–6, the price will be raised in steps to cover individual reactor refurbishment costs, with each increase starting 12 months prior to the start of each individual refurbishment.\n\n7) Q: What was the reason the Bruce A reactor was shut down?\n   A: In 1986, a fuel channel failed while the reactor was shut down; some of the fuel elements were swept into the moderator (calandria) and were difficult to remove.\n\n8) Q: What was the reason for the shutdown?\n   A: The public was informed of the shutdown at the reactor, but not the details of the leakage, since it was not deemed to pose a risk to the public or environment.\n\n9) Q: What is the most common use of carbon-14 in the world?\n   A: Carbon-14 produced in NRU was sold to chemistry, bioscience and environmental labs where it is used as a tracer.\n\n10) Q: How much did the project cost?\n    A: In April 2007, the auditor general reviewed the refurbishment deal In August 2007, estimated cost for the project had grown to $5.\n\n11) Q: How much did it cost to restart units 1 and 2?\n    A: It was determined that while units 1 & 2 could have been restarted without refurbishment, it was economically advantageous to do so, since refurbishment would have been soon required.\n\n12) Q: What is the most common method of neutron scattering?\n    A: Neutron scattering is a technique where a beam of neutrons shines through a sample of material, and depending on how the neutrons scatter from the atoms inside, scientists can determine many details about the crystal structure and movements of the atoms within the sample.\n\n13) Q: What was the risk of fuel failure in the NRU reactor?\n    A: On January 29, 2008, the former President of the CNSC, Linda Keen, testified before a Parliamentary Committee that the risk of fuel failure in the NRU reactor was \"1 in 1000 years\", and claimed this to be a thousand times greater risk than the \"international standard\".\n\n14) Q: What are the main themes of the comments on the draft environmental impact statement?\n    A: CNSC Disposition Table of Public and Indigenous Groups’ Comments on the Draft Environmental Impact Statement-WR-1> The main themes of these comments are Public and Aboriginal Consultation, CNSC Impartiality, and Decommissioning Waste Policies.\n\n15) Q: What happened to the NRX reactor?\n    A: The first incident occurred on December 12, 1952, when there was a power excursion and partial loss of coolant in the NRX reactor, which resulted in significant damage to the core.\n\n16) Q: What was the reason for the shutdown of the reactor?\n    A: In mid-May 2009 a heavy water leak at the base of the reactor vessel was detected, prompting a temporary shutdown of the reactor.\n\n17) Q: What caused the delay in the repair of the tube supports?\n    A: Issues related to the AECL requested design of the tube supports caused repair and delay costs, which exceeded the net worth of the builder Babcock & Wilcox Canada.\n\n18) Q: What was the first experiment that was conducted at Whiteshell Laboratories?\n    A: A similar experiment started the next year in 1974, the ZEUS (Zoological Environment Under Stress) experiment, which set aside six 1-hectare meadow areas in 1974 and carried out long-term radioactive releases to measure the results.\n\n19) Q: What was the cause of the leak?\n    A: The leak was estimated to be 5 kg (5 litres) per hour, a result of corrosion.\n\n20) Q: What is the average price of electricity generated from the Bruce Power reactors?\n    A: The average price per MWh that will be paid to Bruce Power for all electricity generated from 2016 to 2064 (covering the entire refurbishment period for Units 3–6 plus the entire expected remaining post-refurbishment lifetimes of all eight Bruce Power reactors (including the two that were already refurbished) was estimated to be approximately CA$80.\n\nTotal number of question-answer pairs generated: 20\nTraining data saved to train.json\nTesting data saved to test.json\n","output_type":"stream"}]},{"cell_type":"code","source":"# def read_json_file(file_path):\n#     with open(file_path, 'r', encoding='utf-8') as file:\n#         data = json.load(file)\n#     return data\n\n# def print_json_content(file_path):\n#     data = read_json_file(file_path)\n#     print(f\"Content of {file_path}:\")\n#     print(json.dumps(data, indent=4))  # Pretty-print the JSON data\n\n# train_file_path = 'train.json'\n# test_file_path = 'test.json'\n\n# print_json_content(train_file_path)\n# print_json_content(test_file_path)\n\n# import json\n# def count_records(file_path):\n#     with open(file_path, 'r') as file:\n#         data = json.load(file)\n#     return len(data)\n# def main():\n#     train_file_path = 'train.json'\n#     test_file_path = 'test.json' \n#     train_count = count_records(train_file_path)\n#     test_count = count_records(test_file_path)  \n#     print(f\"Number of records in train.json: {train_count}\")\n#     print(f\"Number of records in test.json: {test_count}\")\n# if __name__ == \"__main__\":\n#     main()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:16:52.385762Z","iopub.execute_input":"2024-07-24T16:16:52.386685Z","iopub.status.idle":"2024-07-24T16:16:52.394618Z","shell.execute_reply.started":"2024-07-24T16:16:52.386652Z","shell.execute_reply":"2024-07-24T16:16:52.393672Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Number of records in train.json: 16\nNumber of records in test.json: 4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 5. Model Loader","metadata":{}},{"cell_type":"code","source":"HF_TOKEN = \"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\"\nmodel_name = \"meta-llama/Meta-Llama-3-8B\"\nmax_seq_length = 2048\n\ndef load_model_and_tokenizer():\n    \"\"\"Load the model and tokenizer with configurations.\"\"\"\n    try:\n        print(\"Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=HF_TOKEN)\n        tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.padding_side = \"right\"\n\n        special_tokens = tokenizer.special_tokens_map_extended\n        eos_token = tokenizer.eos_token\n        eos_token_id = tokenizer.eos_token_id\n        \n        print(\"EOS Token:\", eos_token)\n        print(\"EOS Token ID:\", eos_token_id)\n        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n        print(\"Loading model...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=quantization_config,\n            device_map='auto',  \n            low_cpu_mem_usage=True,\n            use_auth_token=HF_TOKEN\n        )\n        print(\"Applying gradient checkpointing and preparing for k-bit training...\")\n        model.gradient_checkpointing_enable()\n        model = prepare_model_for_kbit_training(model)\n        print(\"Model and tokenizer loaded and configured successfully.\")\n        return model, tokenizer\n    except Exception as e:\n        print(\"An error occurred:\", e)\n        return None, None\n\ndef apply_lora_config(model):\n    \"\"\"Apply LoRA configuration to the model.\"\"\"\n    try:\n        print(\"Applying LoRA configuration...\")\n        lora_config = LoraConfig(\n            r=16,\n            lora_alpha=16,\n            target_modules=[\"q_proj\", \"v_proj\"],\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\"\n        )\n        model = get_peft_model(model, lora_config)\n        print(\"LoRA configuration applied successfully.\")\n        return model\n    except Exception as e:\n        print(\"An error occurred while applying LoRA configuration:\", e)\n        return model\nmodel, tokenizer = load_model_and_tokenizer()\n\nif model and tokenizer:\n    model = apply_lora_config(model)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:20:05.708654Z","iopub.execute_input":"2024-07-24T15:20:05.709866Z","iopub.status.idle":"2024-07-24T15:23:11.300883Z","shell.execute_reply.started":"2024-07-24T15:20:05.709828Z","shell.execute_reply":"2024-07-24T15:23:11.299887Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Loading tokenizer...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:778: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42bdf5d7e4d8492c95b2d2bc499a7526"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e493e73670ad44b0831ea25760126943"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1244e2757c2e419cbe8afae807bdb2f5"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"EOS Token: <|end_of_text|>\nEOS Token ID: 128001\nLoading model...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98e08460aa6647eb97cebd460bd7663c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92a57f58abd449d08ae794c3d753f28b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7e7ef2c5466459894a4da472061a60a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfbcf802485e4b1090b10162aeca994c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed7e9e3ae5ef404da7ca0805a8e5efce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df84b11e3517456c98a1e9b3362c67df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba25bf0954a544538041d4b41bb5b5f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e00d4f9e4eca43c7a36dffec0798f557"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fffde0cde8a0488eb44433fb867a3e9d"}},"metadata":{}},{"name":"stdout","text":"Applying gradient checkpointing and preparing for k-bit training...\nModel and tokenizer loaded and configured successfully.\nApplying LoRA configuration...\nLoRA configuration applied successfully.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 6. Dataset Converting to Appropriate Format for Huggingface Transformers","metadata":{}},{"cell_type":"code","source":"def add_questions_key(input_file, output_file):\n    # Read the JSON file\n    with open(input_file, 'r') as infile:\n        data = json.load(infile)        \n    # Format the data by adding the 'questions' key\n    formatted_data = {\n        \"questions\": data\n    }\n\n    # Write the formatted data to a new file\n    with open(output_file, 'w') as outfile:\n        json.dump(formatted_data, outfile, indent=4)\n# File paths for JSON datasets\ntrain_input_file = 'train.json'\ntrain_output_file = 'train_dataset.json'\n\ntest_input_file = 'test.json'\ntest_output_file = 'test_dataset.json'\n\n# Convert training and test datasets to the appropriate format\nadd_questions_key(train_input_file, train_output_file)\nadd_questions_key(test_input_file, test_output_file)\n\nprint(\"JSON files have been formatted and saved successfully.\")\n\nwith open(\"test_dataset.json\") as json_file:\n    test = json.load(json_file)    \nwith open(\"train_dataset.json\") as json_file:\n    train = json.load(json_file)\npd.DataFrame(train[\"questions\"]).head()\npd.DataFrame(test[\"questions\"]).head()\npprint(train[\"questions\"][0], sort_dicts=False)\npprint(test[\"questions\"][0], sort_dicts=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:23:52.613456Z","iopub.execute_input":"2024-07-24T15:23:52.614259Z","iopub.status.idle":"2024-07-24T15:23:52.633285Z","shell.execute_reply.started":"2024-07-24T15:23:52.614221Z","shell.execute_reply":"2024-07-24T15:23:52.632138Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"JSON files have been formatted and saved successfully.\n{'question': 'What was the reason for the shutdown?',\n 'answer': 'Although the leakage was not a concern to the CNSC from a health, '\n           'safety or environmental perspective, AECL made plans for a repair '\n           'to reduce the current leakage rate for operational reasons.'}\n{'question': 'What are the main themes of the comments on the draft '\n             'environmental impact statement?',\n 'answer': 'CNSC Disposition Table of Public and Indigenous Groups’ Comments '\n           'on the Draft Environmental Impact Statement-WR-1> The main themes '\n           'of these comments are Public and Aboriginal Consultation, CNSC '\n           'Impartiality, and Decommissioning Waste Policies.'}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to check data format\ndef check_data_format(data):\n    if \"questions\" not in data or not isinstance(data[\"questions\"], list):\n        raise ValueError(\"The data does not contain the 'questions' key or it is not a list.\")\n\ncheck_data_format(train)\ncheck_data_format(test)\n\n# Define the prompt format\nprompt = \"\"\"Below is a question paired with an answer. Please write a response that appropriately completes the request.\n\n### Question:\n{}\n\n### Answer:\n{}\"\"\"\n\n# Get special tokens and EOS token from the tokenizer\nspecial_tokens = tokenizer.special_tokens_map_extended\neos_token = tokenizer.eos_token\neos_token_id = tokenizer.eos_token_id\n\n# Function to format prompts\ndef formatting_prompts_func(examples):\n    questions = examples[\"question\"]\n    answers = examples[\"answer\"]\n    texts = []\n    for question, answer in zip(questions, answers):\n        # Format the text according to the prompt and append eos_token\n        text = prompt.format(question, answer) + eos_token\n        texts.append(text)\n    return {\"text\": texts}\n\n# Function to convert data into dataset format\ndef create_and_format_dataset(data):\n    dataset_dict = {\n        \"question\": [item[\"question\"] for item in data[\"questions\"]],\n        \"answer\": [item[\"answer\"] for item in data[\"questions\"]],\n    }\n    dataset = Dataset.from_dict(dataset_dict)\n    # Apply the formatting prompts function and remove 'text' column\n    dataset = dataset.map(formatting_prompts_func, batched=True)\n    dataset = dataset.remove_columns([\"text\"])\n    return dataset\n\n# Create and format training and test datasets\ntrain_dataset = create_and_format_dataset(train)\ntest_dataset = create_and_format_dataset(test)\n\n# Create a DatasetDict\ndataset_dict = DatasetDict({\n    'train': train_dataset,\n    'test': test_dataset\n})\n# Check the formatted dataset\nprint(dataset_dict)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:23:55.869672Z","iopub.execute_input":"2024-07-24T15:23:55.870421Z","iopub.status.idle":"2024-07-24T15:23:55.971930Z","shell.execute_reply.started":"2024-07-24T15:23:55.870386Z","shell.execute_reply":"2024-07-24T15:23:55.971023Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae51b45bab6b4edd908e34629d411bf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bb5ceb672be4062969261097b0ca8bc"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 16\n    })\n    test: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 4\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    # Tokenize the input texts\n    inputs = tokenizer(examples['question'], padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt')\n    labels = tokenizer(examples['answer'], padding='max_length', truncation=True, max_length=max_seq_length, return_tensors='pt')\n    \n    # Add labels to inputs\n    inputs['labels'] = labels['input_ids']\n    \n    # Create attention masks for the inputs\n    inputs['attention_mask'] = inputs['attention_mask']\n    \n    return inputs\n\n# Apply preprocessing\ntrain_dataset = train_dataset.map(preprocess_function, batched=True)\ntest_dataset = test_dataset.map(preprocess_function, batched=True)\n# print(train_dataset[0])\n# print(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:24:00.314349Z","iopub.execute_input":"2024-07-24T15:24:00.315057Z","iopub.status.idle":"2024-07-24T15:24:00.822708Z","shell.execute_reply.started":"2024-07-24T15:24:00.315020Z","shell.execute_reply":"2024-07-24T15:24:00.821550Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b54fe6f2bad7479ba2656e3bd532989f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3432f7f998fb41d5a6f4c9a26a6a7766"}},"metadata":{}}]},{"cell_type":"markdown","source":"# 6. Training","metadata":{}},{"cell_type":"code","source":"login(token=\"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\")\nOUTPUT_DIR = \"experiments\"\n\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    fp16=True,\n    save_total_limit=3,\n    logging_steps=10,\n    output_dir=OUTPUT_DIR,\n    max_steps=5,\n    optim=\"paged_adamw_8bit\",\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.05,\n    report_to=\"tensorboard\",\n    evaluation_strategy=\"steps\",\n    eval_steps=10,\n    save_strategy=\"steps\",\n    save_steps=10\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\nmodel.eval()\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\nmodel.config.use_cache = False\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:24:06.347781Z","iopub.execute_input":"2024-07-24T15:24:06.348137Z","iopub.status.idle":"2024-07-24T15:28:12.294905Z","shell.execute_reply.started":"2024-07-24T15:24:06.348109Z","shell.execute_reply":"2024-07-24T15:28:12.293857Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 03:17, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5, training_loss=3.9242427825927733, metrics={'train_runtime': 244.7403, 'train_samples_per_second': 0.082, 'train_steps_per_second': 0.02, 'total_flos': 1846085324636160.0, 'train_loss': 3.9242427825927733, 'epoch': 1.25})"},"metadata":{}}]},{"cell_type":"markdown","source":"# 7. Model Save and Load","metadata":{}},{"cell_type":"code","source":"# def save_model_and_tokenizer(output_dir, model, tokenizer):\n#     model.save_pretrained(output_dir)\n#     tokenizer.save_pretrained(output_dir)\n#     print(f\"Model and tokenizer saved to {output_dir}\")\n\n# # Model and tokenizer save\n# save_model_and_tokenizer(OUTPUT_DIR, model, tokenizer)\n\n# def load_model_and_tokenizer(output_dir):\n#     model = AutoModelForCausalLM.from_pretrained(output_dir)\n#     tokenizer = AutoTokenizer.from_pretrained(output_dir)\n#     print(f\"Model and tokenizer loaded from {output_dir}\")\n#     return model, tokenizer\n# loaded_model, loaded_tokenizer = load_model_and_tokenizer(OUTPUT_DIR)\n# # Model evaluation mode\n# loaded_model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:29:27.130357Z","iopub.execute_input":"2024-07-24T15:29:27.130734Z","iopub.status.idle":"2024-07-24T15:29:27.572293Z","shell.execute_reply.started":"2024-07-24T15:29:27.130704Z","shell.execute_reply":"2024-07-24T15:29:27.571104Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Define the prompt format\nprompt = \"\"\"Below is a question paired with an answer. Please write a response that appropriately completes the request.\n\n### Question:\n{}\n\n### Answer:\n{}\"\"\"\n\ndef generate_answer(question):\n    # Format the prompt with the question\n    formatted_prompt = prompt.format(question, \"\")\n\n    # Tokenize the formatted prompt\n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            inputs[\"input_ids\"],\n            max_length=2048,\n            num_return_sequences=1,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    answer = answer.split('### Answer:')[-1].strip()\n    return answer","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:36:26.693988Z","iopub.execute_input":"2024-07-24T15:36:26.694677Z","iopub.status.idle":"2024-07-24T15:36:26.701390Z","shell.execute_reply.started":"2024-07-24T15:36:26.694643Z","shell.execute_reply":"2024-07-24T15:36:26.700515Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"test_questions = [\n    \"What is the CNSC\"\n]\n\nfor question in test_questions:\n    print(f\"Question: {question}\")\n    print(f\"Answer: {generate_answer(question)}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:37:12.149826Z","iopub.execute_input":"2024-07-24T15:37:12.150444Z","iopub.status.idle":"2024-07-24T15:37:28.518112Z","shell.execute_reply.started":"2024-07-24T15:37:12.150411Z","shell.execute_reply":"2024-07-24T15:37:28.517145Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1797: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"name":"stdout","text":"Question: What is the CNSC\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Answer: The CNSC is the Canadian Nuclear Safety Commission. The CNSC is an independent federal government agency responsible for regulating the use of nuclear energy and materials to protect the health, safety and security of Canadians and the environment.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom datasets import load_dataset, load_metric\nfrom textstat.textstat import textstatistics\n\n# Load metric\nrouge = load_metric(\"rouge\")\n\n# Load model and tokenizer\nmodel_name = \"meta-llama/Meta-Llama-3-8B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=HF_TOKEN)\nmodel = AutoModelForCausalLM.from_pretrained(\"experiments\")  \n\n# Evaluate ROUGE scores\ndef evaluate_rouge(predictions, references):\n    results = rouge.compute(predictions=predictions, references=references)\n    return results\n\n# Calculate readability complexity\ndef calculate_readability(text):\n    complexity = textstatistics().flesch_reading_ease(text)\n    return complexity\n\n# Load TrueQA dataset\ntruthfulqa = load_dataset(\"truthfulqa\")\n\n# Evaluation function for TrueQA\ndef evaluate_truthfulqa(model, tokenizer, dataset):\n    scores = []\n    for item in dataset:\n        question = item[\"question\"]\n        reference_answer = item[\"answer\"]\n        generated_answer = generate_answer(question, model, tokenizer)\n        \n        # Evaluate using ROUGE\n        rouge_result = evaluate_rouge([generated_answer], [reference_answer])\n        scores.append(rouge_result)\n    return scores\n\n# Evaluate on TrueQA validation dataset\ntruthfulqa_scores = evaluate_truthfulqa(model, tokenizer, truthfulqa[\"validation\"])\nprint(truthfulqa_scores)\n\n# MLM pipeline (Optional: If model supports fill-mask)\nmlm_pipeline = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\nmasked_sentence = \"The capital of [MASK] is Paris.\"\nresults = mlm_pipeline(masked_sentence)\nprint(results)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-24T15:44:58.986578Z","iopub.execute_input":"2024-07-24T15:44:58.986945Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdin","text":"The repository for rouge contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/rouge.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:778: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92eabc579d97488e95807b0d9cbd133d"}},"metadata":{}}]}]}
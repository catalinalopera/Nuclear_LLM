{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "975373ae",
   "metadata": {},
   "source": [
    "## Evaluation of the QA pairs for relevance to use in fine-tuning LLM models\n",
    "\n",
    "This code is designed to evaluate the relevance of question-answer (QA) pairs using the OpenAI API, specifically leveraging the GPT-3.5 model. It involves loading QA pairs from a JSON file, running each pair through a predefined relevance evaluation prompt, and then saving the results. The process is broken down into several key steps:\n",
    "\n",
    "*Configuration:* The model, temperature, and maximum tokens for the OpenAI API are set. The OpenAI API key is also configured.\n",
    "\n",
    "*Prompt Template Definition:* A detailed prompt template is defined, instructing the model to flag a QA pair as irrelevant if it meets certain critaria. The prompt includes several examples to guide the model.\n",
    "\n",
    "*Model Initialization:* The LangChain model is initialized with the specified configuration.\n",
    "\n",
    "*Function Definitions:*\n",
    "\n",
    "- load_qa_pairs: This function loads QA pairs from a specified JSON file.\n",
    "- flag_irrelevant_qa_pairs: This function evaluates each QA pair using the OpenAI API, flags it as irrelevant or relevant based on the model's response, and collects the results.\n",
    "- save_flagged_qa_pairs: This function saves the flagged QA pairs to a specified JSON file.\n",
    "\n",
    "*Main Execution:*\n",
    "\n",
    "The script loads the QA pairs from the input JSON file.\n",
    "It processes the first 1000 QA pairs, evaluating their relevance using the model.\n",
    "The flagged results are then saved to an output JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20b2f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flagging QA pairs: 100%|████████████████████| 1000/1000 [06:42<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 flagged QA pairs to flagged_questions_41_50_28Jul4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Configuration\n",
    "model = \"gpt-3.5-turbo-0125\"\n",
    "temperature = 0\n",
    "max_tokens = 100\n",
    "\n",
    "# Set your OpenAI API key\n",
    "api_key = 'API-key'\n",
    "os.environ['OPENAI_API_KEY'] = api_key\n",
    "\n",
    "# Initialize the LangChain model\n",
    "llm = ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)\n",
    "\n",
    "# Define the prompt template to evaluate relevance\n",
    "evaluation_prompt_template = \"\"\"\n",
    "You are an intelligent assistant. Evaluate the relevance of the following question-answer pair for fine-tuning \n",
    "large language models (LLMs). Flag it as irrelevant if it contains any references to sections of a document, tables, figures, appendices, equations, specific parts of a text, or any information that may cause LLMs to hallucinate.\n",
    "\n",
    "Example 1:\n",
    "Question: What does the section on skin contamination in the Radionuclide Information Booklet provide guidance on?\n",
    "Answer: The section on skin contamination provides guidance to licensees on evaluating skin dose as a result of a skin contamination incident.\n",
    "Is this QA pair irrelevant? Yes\n",
    "\n",
    "Example 2:\n",
    "Question: What does the term \"estimation\" refer to in the context of this document?\n",
    "Answer: In the context of this document, estimation refers to two types of approaches to estimating doses: indirect monitoring and dose modelling.\n",
    "Is this QA pair irrelevant? Yes\n",
    "\n",
    "Example 3:\n",
    "Question: What is Appendix D focused on?\n",
    "Answer: Appendix D is focused on radionuclide-specific recommendations related to bioassay measurements and internal dosimetry for Tritium.\n",
    "Is this QA pair irrelevant? Yes\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Is this QA pair irrelevant? Answer with 'Yes' or 'No'.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the prompt\n",
    "evaluation_prompt = PromptTemplate(template=evaluation_prompt_template, input_variables=[\"question\", \"answer\"])\n",
    "evaluation_chain = LLMChain(prompt=evaluation_prompt, llm=llm)\n",
    "\n",
    "def load_qa_pairs(input_file):\n",
    "    \"\"\"Load QA pairs from a JSON file.\"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def flag_irrelevant_qa_pairs(qa_pairs):\n",
    "    \"\"\"Flag irrelevant QA pairs using the OpenAI API.\"\"\"\n",
    "    flagged_qa_pairs = []\n",
    "    for qa in tqdm(qa_pairs, desc=\"Flagging QA pairs\"):\n",
    "        question = qa[\"prompt\"]\n",
    "        answer = qa[\"response\"]\n",
    "        try:\n",
    "            response = evaluation_chain.run({\"question\": question, \"answer\": answer}).strip().lower()\n",
    "            is_irrelevant = response == \"yes\"\n",
    "            flagged_qa_pairs.append({\n",
    "                \"prompt\": question,\n",
    "                \"response\": answer,\n",
    "                \"is_irrelevant\": is_irrelevant\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating QA pair: {e}\")\n",
    "            flagged_qa_pairs.append({\n",
    "                \"prompt\": question,\n",
    "                \"response\": answer,\n",
    "                \"is_irrelevant\": None\n",
    "            })\n",
    "    return flagged_qa_pairs\n",
    "\n",
    "def save_flagged_qa_pairs(flagged_qa_pairs, output_file):\n",
    "    \"\"\"Save flagged QA pairs to a JSON file.\"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(flagged_qa_pairs, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Saved {len(flagged_qa_pairs)} flagged QA pairs to {output_file}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"/Users/zarinadossayeva/Desktop/WIL_LLM/CNSC_QA_pairs_JSON/CNSC_QA_pairs/CNSC_QA_pairs_41_50.json\"\n",
    "    output_file = \"flagged_questions_41_50_28Jul4.json\"\n",
    "    \n",
    "    # Load the QA pairs from the input file\n",
    "    qa_pairs = load_qa_pairs(input_file)\n",
    "    \n",
    "    # Flag irrelevant QA pairs\n",
    "    #flagged_qa_pairs = flag_irrelevant_qa_pairs(qa_pairs)\n",
    "        # Process only the first 1000 QA pairs\n",
    "    first_1000_qa_pairs = qa_pairs[:1000]\n",
    "    \n",
    "    # Flag irrelevant QA pairs\n",
    "    flagged_qa_pairs = flag_irrelevant_qa_pairs(first_1000_qa_pairs)\n",
    "    \n",
    "    # Save the flagged QA pairs to the output file\n",
    "    save_flagged_qa_pairs(flagged_qa_pairs, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
